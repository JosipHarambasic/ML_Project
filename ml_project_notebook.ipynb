{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ml_project_notebook.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3H2YSwnoPX6y",
    "outputId": "f45acd77-7f08-4884-82d4-43b44d6e82f2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "id": "xhX8N-rzP9Rr"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NNoo1k3ZmBBO",
    "outputId": "f0dfd4f2-c2fa-4485-b0c0-162b6ad0aebc"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from verstack import NaNImputer\n",
    "from pyod.models.iforest import IForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ],
   "metadata": {
    "id": "MPJ1fGs7Qhgy",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # do not show warnings"
   ],
   "metadata": {
    "id": "oo0QnAcQjjZG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helpers"
   ],
   "metadata": {
    "id": "xpuGNAJSP_Br"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model related"
   ],
   "metadata": {
    "id": "O1oIE84yslGV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def test_vanilla(X, y):\n",
    "    ETC = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
    "    RFC = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    GBC = GradientBoostingClassifier(random_state=42)\n",
    "    #LGBMC = LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "    #XGBC = XGBClassifier(seed=42)\n",
    "    CBC = CatBoostClassifier(random_state=42, verbose=0)\n",
    "    KNN = KNeighborsClassifier(n_jobs=-1)\n",
    "    SGDC = SGDClassifier(random_state=42, n_jobs=-1)\n",
    "    MLP = MLPClassifier(random_state=42)\n",
    "\n",
    "    names = ['ETC', 'RFC', 'GBC', 'CBC', 'KNN', 'SGDC', \"MLP\"]\n",
    "    models = [ETC, RFC, GBC, CBC, KNN, SGDC, MLP]\n",
    "\n",
    "    for name, model in zip(names, models):\n",
    "        metric = 'Accuracy'\n",
    "        cv = RepeatedKFold(n_splits=5, \n",
    "                          n_repeats=1, \n",
    "                          random_state=42)\n",
    "        grid = GridSearchCV(estimator=model, \n",
    "                            param_grid={}, \n",
    "                            scoring=metric.lower(),\n",
    "                            cv=cv, \n",
    "                            verbose=0,\n",
    "                            n_jobs=-1)\n",
    "        grid.fit(X, y)\n",
    "        print(f\"{metric} for {name}: {grid.best_score_}\")"
   ],
   "metadata": {
    "id": "HSc-4dDxsnuV"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_feature_importances(X, \n",
    "                            y, \n",
    "                            max_features: int) -> dict:\n",
    "    ETC = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
    "    RFC = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    GBC = GradientBoostingClassifier(random_state=42)\n",
    "    #LGBMC = LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "    #XGBC = XGBClassifier(seed=42)\n",
    "    CBC = CatBoostClassifier(random_state=42, verbose=0)\n",
    "    SGDC = SGDClassifier(random_state=42, n_jobs=-1)\n",
    "    MLP = MLPClassifier(random_state=42)\n",
    "\n",
    "    names = ['ETC', 'RFC', 'GBC', 'CBC', 'SGDC', \"MLP\"]\n",
    "    models = [ETC, RFC, GBC, CBC, SGDC, MLP]\n",
    "\n",
    "    feature_importances_0 = np.zeros((7, max_features))\n",
    "    feature_importances_1 = np.zeros((7, max_features//2))\n",
    "    feature_importances_2 = np.zeros((7, max_features//4))\n",
    "    \n",
    "    for i, name, model in zip(range(len(models)), names, models):\n",
    "        print(f'Fitting {name}...', end=' ')\n",
    "        model.fit(X, y)\n",
    "        print('Done')\n",
    "        selector_0 = SelectFromModel(model, \n",
    "                                     threshold=-np.inf,\n",
    "                                     prefit=True,\n",
    "                                     max_features=max_features)\n",
    "        feature_importances_0[i] = selector_0.get_support(indices=True)\n",
    "\n",
    "        selector_1 = SelectFromModel(model, \n",
    "                                     threshold=-np.inf,\n",
    "                                     prefit=True,\n",
    "                                     max_features=max_features//2)\n",
    "        feature_importances_1[i] = selector_1.get_support(indices=True)\n",
    "\n",
    "        selector_2 = SelectFromModel(model, \n",
    "                                     threshold=-np.inf,\n",
    "                                     prefit=True,\n",
    "                                     max_features=max_features//4)\n",
    "        feature_importances_2[i] = selector_2.get_support(indices=True)\n",
    "\n",
    "    tfi_0 = feature_importances_0.flatten()\n",
    "    tfi_1 = feature_importances_1.flatten()\n",
    "    tfi_2 = feature_importances_2.flatten()\n",
    "\n",
    "    u_0 = np.unique(tfi_0)\n",
    "    u_1 = np.unique(tfi_1)\n",
    "    u_2 = np.unique(tfi_2)\n",
    "\n",
    "    fi_dict = {f'feature_importances_{max_features}': u_0.astype(int),\n",
    "               f'feature_importances_{max_features//2}': u_1.astype(int),\n",
    "               f'feature_importances_{max_features//4}': u_2.astype(int)}\n",
    "\n",
    "    return fi_dict"
   ],
   "metadata": {
    "id": "kt2K5JTOmaT9"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset class"
   ],
   "metadata": {
    "id": "Mt0E-hfuJK0T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DataSet:\n",
    "    def __init__(self,\n",
    "                 path: str = 'ML_Project/dataset',\n",
    "                 years: list = [2014, 2015, 2016, 2017, 2018],\n",
    "                 nan_handler: list = ['threshold'],\n",
    "                 zero_handler: list = ['threshold', 'replace'],\n",
    "                 imputation_handler: str = 'XGB',\n",
    "                 outlier_handler: list = ['quantile', 'IForest'],\n",
    "                 config_dict: dict = {}\n",
    "                 ):\n",
    "        self.path = path\n",
    "        self.years = years\n",
    "        self.nan_handler = nan_handler\n",
    "        self.zero_handler = zero_handler\n",
    "        self.imputation_handler = imputation_handler\n",
    "        self.outlier_handler = outlier_handler\n",
    "        self.config_dict = config_dict\n",
    "\n",
    "        self.df = None\n",
    "        self.year = None\n",
    "        self.init_shape = None\n",
    "        self.class_col = None\n",
    "        self.sector_col = None\n",
    "        self.sector_ids = None\n",
    "        self.sector_mapper = None\n",
    "\n",
    "        self.dfs = []\n",
    "        self.intersec_cols = None\n",
    "\n",
    "    def create_dataset(self):\n",
    "        # loop over all years\n",
    "        for year in self.years:\n",
    "            self.__print_sep(60, '#', '\\n')\n",
    "\n",
    "            # generate single dataframe\n",
    "            self.prepare_single_dataframe(year)\n",
    "\n",
    "            # append dataframe\n",
    "            self.dfs.append(self.df)\n",
    "\n",
    "        # print seperators\n",
    "        self.__print_sep(60, '#')\n",
    "        self.__print_sep(60, '#', '\\n')\n",
    "\n",
    "        # get intersecting columns\n",
    "        self.get_intersecting_columns()\n",
    "\n",
    "        # concatenate dataframes to one\n",
    "        df = self.concat_intersecting_dfs()\n",
    "\n",
    "        # factorize 'Sector' column\n",
    "        df = self.factorize_col(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prepare_single_dataframe(self, year: int):\n",
    "        # load data\n",
    "        self.load_df(year)\n",
    "\n",
    "        # store and drop 'Class' column\n",
    "        self.store_drop_class_col()\n",
    "\n",
    "        # handle NaNs\n",
    "        self.handle_nans()\n",
    "\n",
    "        # handle zeros\n",
    "        self.handle_zeros()\n",
    "\n",
    "        # factorize 'Sector' column and store its mapper\n",
    "        self.store_factorize_sector_col()\n",
    "\n",
    "        # impute NaNs\n",
    "        self.impute_nans()\n",
    "\n",
    "        # handle outliers\n",
    "        self.handle_outliers()\n",
    "\n",
    "        # drop VAR column\n",
    "        self.drop_var_col()\n",
    "\n",
    "        # scale data\n",
    "        self.scale_data()\n",
    "\n",
    "        # convert to float 32 and join\n",
    "        self.convert_join()\n",
    "\n",
    "        # map 'Sector' back to string representation\n",
    "        self.map_sector_inv()\n",
    "\n",
    "    def load_df(self, year: int):\n",
    "        self.__print_header(f'LOADING {year}')\n",
    "\n",
    "        self.year = year\n",
    "        print(f'Loading {year}_Financial_Data.csv into a DataFrame', end=' - ')\n",
    "        self.df = pd.read_csv(os.path.join(self.path, f'{year}_Financial_Data.csv'),\n",
    "                              index_col=0)\n",
    "        print('COMPLETE')\n",
    "        self.init_shape = self.df.shape\n",
    "\n",
    "        # drop rows with no info\n",
    "        print('Dropping rows with NaNs only', end=' - ')\n",
    "        rows = self.df.shape[0]\n",
    "        self.df.dropna(how='all', inplace=True)\n",
    "        print(f'{self.df.shape[0] - rows} rows dropped - COMPLETE')\n",
    "\n",
    "        self.__print_summary(f'Initial DataFrame shape: {self.df.shape}')\n",
    "\n",
    "    def store_drop_class_col(self):\n",
    "        self.class_col = self.df['Class'].astype('int8')\n",
    "        self.df.drop(columns=['Class'], inplace=True)\n",
    "\n",
    "    def store_factorize_sector_col(self):\n",
    "        self.sector_col = self.df['Sector']\n",
    "        self.sector_ids, self.sector_mapper = pd.factorize(self.df['Sector'])\n",
    "        self.df['Sector'] = self.sector_ids\n",
    "\n",
    "    def factorize_col(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        sector_ids, self.sector_mapper = pd.factorize(df['Sector'])\n",
    "        df['Sector'] = sector_ids\n",
    "\n",
    "        return df\n",
    "\n",
    "    def handle_nans(self, round: int = 1):\n",
    "        if round == 1 and self.nan_handler[0]:\n",
    "            self.__print_header(f'HANDLING NANS')\n",
    "\n",
    "        if 'threshold' in self.nan_handler:\n",
    "            nans_thres = self.config_dict[self.year]['nans_thres']\n",
    "            print(f'Dropping columns with more than {int(nans_thres * 100)}% NaNs', end=' - ')\n",
    "            cols = self.df.shape[1]\n",
    "            self.df = self.df.loc[:, self.df.isnull().mean() < nans_thres]  # drop nans\n",
    "            print(f'{cols - self.df.shape[1]} columns dropped - COMPLETE')\n",
    "            self.__print_sep(60, '~', '\\n')\n",
    "\n",
    "    def handle_zeros(self):\n",
    "        self.__print_header(f'HANDLING ZEROS')\n",
    "\n",
    "        if 'threshold' in self.zero_handler:\n",
    "            zeros_thres = self.config_dict[self.year]['zeros_thres']\n",
    "            print(f'Dropping columns with more than {int(zeros_thres * 100)}% zeros', end=' - ')\n",
    "            cols = self.df.shape[1]\n",
    "            self.df = self.df.loc[:, (self.df == 0).mean() < zeros_thres]  # drop zeros\n",
    "            print(f'{cols - self.df.shape[1]} columns dropped - COMPLETE')\n",
    "\n",
    "        if 'replace' in self.zero_handler:\n",
    "            print(f'Replacing zeros with NaNs', end=' - ')\n",
    "            z_count = (self.df == 0).sum().sum()\n",
    "            self.df = self.df.replace({0: np.nan})\n",
    "            z_count_after = (self.df == 0).sum().sum()\n",
    "            print(f'{z_count - z_count_after} zeros replaced - COMPLETE')\n",
    "\n",
    "        if 'replace' in self.zero_handler and 'drop_again' in self.zero_handler:\n",
    "            self.handle_nans(round=2)\n",
    "\n",
    "        da = self.init_shape[1] - self.df.shape[1]\n",
    "        sc = self.df.shape\n",
    "        self.__print_summary(f'Total amount of columns dropped: {da} - Current shape: {sc}')\n",
    "\n",
    "    def impute_nans(self):\n",
    "        self.__print_header(f'IMPUTE NANS')\n",
    "\n",
    "        if self.imputation_handler == 'KNN':\n",
    "            print(f'Imputing {self.df.isnull().sum().sum()} NaNs with KNN', end=' - ')\n",
    "            imputer = KNNImputer(n_neighbors=20, weights='distance', metric='nan_euclidean', copy=True)\n",
    "            np_imp = imputer.fit_transform(self.df)\n",
    "            self.df = pd.DataFrame(np_imp, columns=self.df.columns, index=self.df.index)\n",
    "            print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
    "\n",
    "        elif self.imputation_handler == 'XGB':\n",
    "            print(f'Imputing {self.df.isnull().sum().sum()} NaNs with XGB', end=' - ')\n",
    "            imputer = NaNImputer(verbose=False, multiprocessing_load=1)\n",
    "            self.df = imputer.impute(self.df)\n",
    "            print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
    "            cols = self.df.shape[1]\n",
    "            self.df.dropna(axis=1, inplace=True)  # remove columns that still have NaNs inside\n",
    "            print(f'Removing {cols - self.df.shape[1]} columns such that', end=' ')\n",
    "            print(f'{self.df.isnull().sum().sum()} NaNs are left - COMPLETE')\n",
    "\n",
    "        elif self.imputation_handler == 'mean':\n",
    "            print(f'Imputing NaNs with mean')\n",
    "\n",
    "            # drop rows where the 'Revenue' is unknown (NaNs)\n",
    "            print(f'Dropping rows that have no information about revenue', end=' - ')\n",
    "            rows = self.df.shape[0]\n",
    "            self.df.drop(self.df[self.df['Revenue'].isnull()].index, inplace=True)\n",
    "            print(f'{rows - self.df.shape[0]} rows dropped - COMPLETE')\n",
    "\n",
    "            # introduce new column that describes revenue ranges\n",
    "            range_cond = [(self.df['Revenue'] <= 1e6),\n",
    "                          (self.df['Revenue'] > 1e6) & (self.df['Revenue'] <= 1e7),\n",
    "                          (self.df['Revenue'] > 1e7) & (self.df['Revenue'] <= 1e8),\n",
    "                          (self.df['Revenue'] > 1e8) & (self.df['Revenue'] <= 1e9),\n",
    "                          (self.df['Revenue'] > 1e9)]\n",
    "            self.df['Revenue Range'] = np.select(range_cond, [0, 1, 2, 3, 4])\n",
    "\n",
    "            # store sector column to add it after groupby\n",
    "            sector_col = self.df['Sector']\n",
    "\n",
    "            # group by sector and then revenue range and impute nans with means\n",
    "            nans_count = self.df.isnull().sum().sum()\n",
    "            self.df = self.df.groupby(['Sector', 'Revenue Range']).transform(lambda x: x.fillna(x.mean()))\n",
    "            nans_count_after = self.df.isnull().sum().sum()\n",
    "            print(f'{nans_count - nans_count_after} NaNs imputed', end=' - ')\n",
    "            print(f'{nans_count_after} NaNs left - COMPLETE')\n",
    "\n",
    "            # add sector column again\n",
    "            self.df['Sector'] = sector_col\n",
    "\n",
    "            # if there are still NaNs left, remove the corresponding row(s)\n",
    "            if nans_count_after > 0:\n",
    "                print(f'Dropping rows with remaining NaNs', end=' - ')\n",
    "                row_count = self.df.shape[0]\n",
    "                self.df = self.df.dropna(axis=0)\n",
    "                print(f'{row_count - self.df.shape[0]} row(s) dropped', end=' - ')\n",
    "                print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
    "\n",
    "        self.__print_sep(60, '~', '\\n')\n",
    "\n",
    "    def handle_outliers(self):\n",
    "        self.__print_header('HANDLE OUTLIERS')\n",
    "\n",
    "        if 'quantile' in self.outlier_handler:\n",
    "            rows_before = self.df.shape[0]\n",
    "            cut_lower = self.config_dict[self.year]['cut_lower']\n",
    "            cut_upper = self.config_dict[self.year]['cut_upper']\n",
    "            # drop rows with an unnaturally high price variance\n",
    "            print(f'Dropping rows with a price variance outside the {cut_lower} - {cut_upper} quantile range',\n",
    "                  end=' - ')\n",
    "            col = f'{self.year + 1} PRICE VAR [%]'\n",
    "            outs = self.df[col].between(self.df[col].quantile(cut_lower),\n",
    "                                        self.df[col].quantile(cut_upper))\n",
    "            self.df.drop(self.df[~outs].index, inplace=True)\n",
    "            print(f'{rows_before - self.df.shape[0]} rows dropped - COMPLETE')\n",
    "\n",
    "        if 'IForest' in self.outlier_handler:\n",
    "            print('Using Isolation Forest to detect outliers', end=' - ')\n",
    "            contamination = self.config_dict[self.year]['IForest_contamination']\n",
    "            clf = IForest(contamination=contamination,\n",
    "                          random_state=42,\n",
    "                          n_jobs=-1)\n",
    "            clf.fit(self.df.values)\n",
    "            y_pred = clf.predict(self.df.values)\n",
    "            idx_y_pred = [i for i in range(self.df.shape[0]) if y_pred[i] == 0]\n",
    "            self.df = self.df.iloc[idx_y_pred, :]\n",
    "            print(f'{sum(y_pred)} outliers removed - COMPLETE')\n",
    "\n",
    "        self.__print_sep(60, '~', '\\n')\n",
    "\n",
    "    def scale_data(self):\n",
    "        self.__print_header('SCALING DATA')\n",
    "\n",
    "        print(f'Scaling data', end=' - ')\n",
    "        sector_col = self.df['Sector'] if 'Sector' in self.df.columns else None\n",
    "        scaler = StandardScaler()\n",
    "        np_scaled = scaler.fit_transform(self.df)\n",
    "        self.df = pd.DataFrame(np_scaled,\n",
    "                               columns=self.df.columns,\n",
    "                               index=self.df.index)\n",
    "\n",
    "        if sector_col is not None:\n",
    "            self.df['Sector'] = sector_col\n",
    "        print('COMPLETE')\n",
    "\n",
    "    def drop_var_col(self):\n",
    "        self.df.drop(columns=[f'{self.year + 1} PRICE VAR [%]'], inplace=True)\n",
    "\n",
    "    def convert_join(self):\n",
    "        # convert to float32\n",
    "        self.df = self.df.astype('float32')\n",
    "\n",
    "        # join 'Class' column\n",
    "        self.df = self.df.join(self.class_col)\n",
    "\n",
    "        self.__print_summary(f'Final DataFrame shape: {self.df.shape}')\n",
    "\n",
    "    def map_sector_inv(self):\n",
    "        mapper = {i: sector for i, sector in enumerate(self.sector_mapper)}\n",
    "        self.df['Sector'] = self.df['Sector'].apply(lambda x: mapper[x])\n",
    "\n",
    "    def get_intersecting_columns(self):\n",
    "        df1 = self.dfs[0].columns\n",
    "        df2 = self.dfs[1].columns\n",
    "        df3 = self.dfs[2].columns\n",
    "        df4 = self.dfs[3].columns\n",
    "        df5 = self.dfs[4].columns\n",
    "\n",
    "        self.intersec_cols = df1 & df2 & df3 & df4 & df5\n",
    "        print(f'Found {len(self.intersec_cols)} intersecting columns!')\n",
    "\n",
    "    def concat_intersecting_dfs(self):\n",
    "        df1 = self.dfs[0][self.intersec_cols]\n",
    "        df2 = self.dfs[1][self.intersec_cols]\n",
    "        df3 = self.dfs[2][self.intersec_cols]\n",
    "        df4 = self.dfs[3][self.intersec_cols]\n",
    "        df5 = self.dfs[4][self.intersec_cols]\n",
    "\n",
    "        df = pd.concat([df1, df2, df3, df4, df5])\n",
    "        print(f'Concatenated DataFrame into shape: {df.shape}')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def __print_summary(self, info: str):\n",
    "        self.__print_sep()\n",
    "        print(info)\n",
    "        self.__print_sep(60, '~', '\\n')\n",
    "\n",
    "    def __print_sep(self, n: int = 60, c: str = '-', nl: str = ''):\n",
    "        print(n * c + nl)\n",
    "\n",
    "    def __print_header(self, header: str):\n",
    "        rem = 60 - 25 - 2 - len(header)\n",
    "        print(f'{25 * \"~\"} {header} {rem * \"~\"}')"
   ],
   "metadata": {
    "id": "Qb_W1JWaj498"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare approaches"
   ],
   "metadata": {
    "id": "HNHa5JNemgs-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set config dictionary"
   ],
   "metadata": {
    "id": "KhDP7N_Bs9Ry"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config_dict = {\n",
    "    2014: {\n",
    "        'nans_thres': 0.08, \n",
    "        'zeros_thres': 0.08,\n",
    "        'cut_lower': 0.01,\n",
    "        'cut_upper': 0.99,\n",
    "        'IForest_contamination': 0.02\n",
    "    },\n",
    "    2015: {\n",
    "        'nans_thres': 0.10, \n",
    "        'zeros_thres': 0.08,\n",
    "        'cut_lower': 0.01,\n",
    "        'cut_upper': 0.99,\n",
    "        'IForest_contamination': 0.02\n",
    "    },\n",
    "    2016: {\n",
    "        'nans_thres': 0.16, \n",
    "        'zeros_thres': 0.08,\n",
    "        'cut_lower': 0.01,\n",
    "        'cut_upper': 0.99,\n",
    "        'IForest_contamination': 0.02\n",
    "    },\n",
    "    2017: {\n",
    "        'nans_thres': 0.16, \n",
    "        'zeros_thres': 0.08,\n",
    "        'cut_lower': 0.01,\n",
    "        'cut_upper': 0.99,\n",
    "        'IForest_contamination': 0.02\n",
    "    },\n",
    "    2018: {\n",
    "        'nans_thres': 0.08, \n",
    "        'zeros_thres': 0.08,\n",
    "        'cut_lower': 0.01,\n",
    "        'cut_upper': 0.99,\n",
    "        'IForest_contamination': 0.02\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "id": "4OwwwrQwgiO-"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Approach 1\n",
    "\n",
    "1.   *NaN threshold*\n",
    "2.   *zero threshold*\n",
    "3.   *mean imputation*\n",
    "4.   *outlier quantile cut*\n",
    "\n"
   ],
   "metadata": {
    "id": "otz1ng9emkbF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_1 = DataSet(config_dict=config_dict,\n",
    "                    nan_handler = ['threshold'],\n",
    "                    zero_handler = ['threshold'],\n",
    "                    imputation_handler = 'mean',\n",
    "                    outlier_handler = ['quantile'])\n",
    "\n",
    "# generate DataFrame\n",
    "df_1 = dataset_1.create_dataset()\n",
    "\n",
    "# use full df for cross validation vanilla test\n",
    "X_1, y_1 = df_1.drop(columns=['Class']), df_1['Class']"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "XyMm5zQ8gZnV",
    "outputId": "ebee1930-131c-4113-b1de-b71e443f4411"
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2014 ~~~~~~~~~~~~~~~~~~~~~\n",
      "Loading 2014_Financial_Data.csv into a DataFrame - COMPLETE\n",
      "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Initial DataFrame shape: (3808, 224)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
      "Dropping columns with more than 8% NaNs - 89 columns dropped - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
      "Dropping columns with more than 8% zeros - 60 columns dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Total amount of columns dropped: 150 - Current shape: (3808, 74)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
      "Imputing NaNs with mean\n",
      "Dropping rows that have no information about revenue - 44 rows dropped - COMPLETE\n",
      "8163 NaNs imputed - 0 NaNs left - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
      "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 76 rows dropped - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
      "Scaling data - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Final DataFrame shape: (3688, 74)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "############################################################\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2015 ~~~~~~~~~~~~~~~~~~~~~\n",
      "Loading 2015_Financial_Data.csv into a DataFrame - COMPLETE\n",
      "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Initial DataFrame shape: (4120, 224)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
      "Dropping columns with more than 10% NaNs - 68 columns dropped - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
      "Dropping columns with more than 8% zeros - 67 columns dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Total amount of columns dropped: 136 - Current shape: (4120, 88)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
      "Imputing NaNs with mean\n",
      "Dropping rows that have no information about revenue - 67 rows dropped - COMPLETE\n",
      "16665 NaNs imputed - 0 NaNs left - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
      "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 82 rows dropped - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
      "Scaling data - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Final DataFrame shape: (3971, 88)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "############################################################\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2016 ~~~~~~~~~~~~~~~~~~~~~\n",
      "Loading 2016_Financial_Data.csv into a DataFrame - COMPLETE\n",
      "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Initial DataFrame shape: (4797, 224)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
      "Dropping columns with more than 16% NaNs - 97 columns dropped - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
      "Dropping columns with more than 8% zeros - 61 columns dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Total amount of columns dropped: 159 - Current shape: (4797, 65)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
      "Imputing NaNs with mean\n",
      "Dropping rows that have no information about revenue - 489 rows dropped - COMPLETE\n",
      "9768 NaNs imputed - 10 NaNs left - COMPLETE\n",
      "Dropping rows with remaining NaNs - 1 row(s) dropped - 0 NaNs left - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
      "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 88 rows dropped - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
      "Scaling data - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Final DataFrame shape: (4219, 65)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "############################################################\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2017 ~~~~~~~~~~~~~~~~~~~~~\n",
      "Loading 2017_Financial_Data.csv into a DataFrame - COMPLETE\n",
      "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Initial DataFrame shape: (4960, 224)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
      "Dropping columns with more than 16% NaNs - 145 columns dropped - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
      "Dropping columns with more than 8% zeros - 31 columns dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Total amount of columns dropped: 177 - Current shape: (4960, 47)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
      "Imputing NaNs with mean\n",
      "Dropping rows that have no information about revenue - 525 rows dropped - COMPLETE\n",
      "5506 NaNs imputed - 0 NaNs left - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
      "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 90 rows dropped - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
      "Scaling data - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Final DataFrame shape: (4345, 47)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "############################################################\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2018 ~~~~~~~~~~~~~~~~~~~~~\n",
      "Loading 2018_Financial_Data.csv into a DataFrame - COMPLETE\n",
      "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Initial DataFrame shape: (4392, 224)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
      "Dropping columns with more than 8% NaNs - 62 columns dropped - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
      "Dropping columns with more than 8% zeros - 70 columns dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Total amount of columns dropped: 133 - Current shape: (4392, 91)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
      "Imputing NaNs with mean\n",
      "Dropping rows that have no information about revenue - 46 rows dropped - COMPLETE\n",
      "14056 NaNs imputed - 0 NaNs left - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
      "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 88 rows dropped - COMPLETE\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
      "Scaling data - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Final DataFrame shape: (4258, 91)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "############################################################\n",
      "############################################################\n",
      "\n",
      "Found 42 intersecting columns!\n",
      "Concatenated DataFrame into shape: (20481, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_33321/2900163752.py:287: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  self.intersec_cols = df1 & df2 & df3 & df4 & df5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_vanilla(X_1, y_1)"
   ],
   "metadata": {
    "id": "Cw4By1i5oz-e"
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ETC: 0.7009426914891994\n",
      "Accuracy for RFC: 0.7019196115297779\n",
      "Accuracy for GBC: 0.6988923869599708\n",
      "Accuracy for CBC: 0.7033840858211802\n",
      "Accuracy for KNN: 0.6094914867204662\n",
      "Accuracy for SGDC: 0.5520240968048267\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Approach 2\n",
    "\n",
    "\n",
    "1.   *zero threshold*\n",
    "2.   *replace zeros with NaNs*\n",
    "3.   *KNN imputation*\n",
    "4.   *outlier quantile cut*\n",
    "5.   *Isolation Forest outlier handling*\n",
    "\n"
   ],
   "metadata": {
    "id": "H4SGEHkIo7kQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_2 = DataSet(config_dict=config_dict,\n",
    "                    nan_handler = [],\n",
    "                    zero_handler = ['threshold', 'replace'],\n",
    "                    imputation_handler = 'KNN',\n",
    "                    outlier_handler = ['quantile', 'IForest'])\n",
    "\n",
    "# generate DataFrame\n",
    "df_2 = dataset_2.create_dataset()\n",
    "\n",
    "# use full df for cross validation vanilla test\n",
    "X_2, y_2 = df_2.drop(columns=['Class']), df_2['Class']"
   ],
   "metadata": {
    "id": "KNjNTA_PqSil"
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2014 ~~~~~~~~~~~~~~~~~~~~~\n",
      "Loading 2014_Financial_Data.csv into a DataFrame - COMPLETE\n",
      "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Initial DataFrame shape: (3808, 224)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_33321/2252005290.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# generate DataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mdf_2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdataset_2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m# use full df for cross validation vanilla test\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_33321/2900163752.py\u001B[0m in \u001B[0;36mcreate_dataset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0;31m# generate single dataframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprepare_single_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0myear\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0;31m# append dataframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_33321/2900163752.py\u001B[0m in \u001B[0;36mprepare_single_dataframe\u001B[0;34m(self, year)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0;31m# handle NaNs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandle_nans\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m         \u001B[0;31m# handle zeros\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_33321/2900163752.py\u001B[0m in \u001B[0;36mhandle_nans\u001B[0;34m(self, round)\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mhandle_nans\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mround\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0mround\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnan_handler\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    125\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__print_header\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'HANDLING NANS'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_vanilla(X_2, y_2)"
   ],
   "metadata": {
    "id": "MKZb3loZrFCr"
   },
   "execution_count": 12,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_33321/3576209638.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtest_vanilla\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'X_2' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Approach 3\n",
    "\n",
    "1.   *replace zeros with NaNs*\n",
    "3.   *XGB imputation*\n",
    "4.   *outlier quantile cut*\n",
    "5.   *Isolation Forest outlier handling*\n"
   ],
   "metadata": {
    "id": "cKkOx65LrH0b"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_3 = DataSet(config_dict=config_dict,\n",
    "                    nan_handler = [],\n",
    "                    zero_handler = ['replace'],\n",
    "                    imputation_handler = 'XGB',\n",
    "                    outlier_handler = ['quantile', 'IForest'])\n",
    "\n",
    "# generate DataFrame\n",
    "df_3 = dataset_3.create_dataset()\n",
    "\n",
    "# use full df for cross validation vanilla test\n",
    "X_3, y_3= df_3.drop(columns=['Class']), df_3['Class']"
   ],
   "metadata": {
    "id": "L-dLVG_8rceJ"
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2014 ~~~~~~~~~~~~~~~~~~~~~\n",
      "Loading 2014_Financial_Data.csv into a DataFrame - COMPLETE\n",
      "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
      "------------------------------------------------------------\n",
      "Initial DataFrame shape: (3808, 224)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_33321/370729561.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# generate DataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mdf_3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdataset_3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m# use full df for cross validation vanilla test\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_33321/2900163752.py\u001B[0m in \u001B[0;36mcreate_dataset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0;31m# generate single dataframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprepare_single_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0myear\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0;31m# append dataframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_33321/2900163752.py\u001B[0m in \u001B[0;36mprepare_single_dataframe\u001B[0;34m(self, year)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0;31m# handle NaNs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandle_nans\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m         \u001B[0;31m# handle zeros\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_33321/2900163752.py\u001B[0m in \u001B[0;36mhandle_nans\u001B[0;34m(self, round)\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mhandle_nans\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mround\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0mround\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnan_handler\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    125\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__print_header\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'HANDLING NANS'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_vanilla(X_3, y_3)"
   ],
   "metadata": {
    "id": "uOvsf0o5ro21"
   },
   "execution_count": 14,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_33321/3097371599.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtest_vanilla\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'X_3' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Approach 4\n",
    "\n",
    "1.   *NaN threshold*\n",
    "2.   *zero threshold*\n",
    "3.   *replace zeros with NaNs*\n",
    "4.   *NaN threshold again*\n",
    "5.   *KNN imputation*\n",
    "6.   *outlier quantile cut*\n",
    "7.   *Isolation Forest outlier handling*"
   ],
   "metadata": {
    "id": "Khb0Dm11rpyq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_4 = DataSet(config_dict=config_dict,\n",
    "                    nan_handler = [],\n",
    "                    zero_handler = ['replace'],\n",
    "                    imputation_handler = 'XGB',\n",
    "                    outlier_handler = ['quantile', 'IForest'])\n",
    "\n",
    "# generate DataFrame\n",
    "df_4 = dataset_4.create_dataset()\n",
    "\n",
    "# use full df for cross validation vanilla test\n",
    "X_4, y_4= df_4.drop(columns=['Class']), df_4['Class']"
   ],
   "metadata": {
    "id": "9STn0QbfsV6D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_vanilla(X_4, y_4)"
   ],
   "metadata": {
    "id": "Ay4t_2VOsf8p"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}