{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_project_notebook (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosipHarambasic/ML_Project/blob/master/ml_project_notebook_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H2YSwnoPX6y",
        "outputId": "620320c8-4ae9-443a-c0af-c72dd84d2bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ML_Project'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (112/112), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 112 (delta 44), reused 91 (delta 23), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (112/112), 44.09 MiB | 12.15 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "xhX8N-rzP9Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install verstack\n",
        "!pip install catboost\n",
        "!pip install pyod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NNoo1k3ZmBBO",
        "outputId": "55c88a8c-2068-4e65-ed7f-4bb43becb933"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting verstack\n",
            "  Downloading verstack-3.0.3.tar.gz (9.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from verstack) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from verstack) (1.21.5)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (from verstack) (0.90)\n",
            "Collecting scikit-learn==1.0.1\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 3.0 MB/s \n",
            "\u001b[?25hCollecting lightgbm==3.3.0\n",
            "  Downloading lightgbm-3.3.0-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 55.5 MB/s \n",
            "\u001b[?25hCollecting optuna==2.10.0\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[K     |████████████████████████████████| 308 kB 58.7 MB/s \n",
            "\u001b[?25hCollecting plotly==5.3.1\n",
            "  Downloading plotly-5.3.1-py2.py3-none-any.whl (23.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9 MB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from verstack) (3.2.2)\n",
            "Collecting python-dateutil==2.8.1\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 43.1 MB/s \n",
            "\u001b[?25hCollecting holidays==0.11.3.1\n",
            "  Downloading holidays-0.11.3.1-py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 56.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (from verstack) (0.14.0)\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 489.6 MB 22 kB/s \n",
            "\u001b[?25hCollecting keras==2.7.0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 56.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: convertdate>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from holidays==0.11.3.1->verstack) (2.4.0)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays==0.11.3.1->verstack) (0.2.1)\n",
            "Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays==0.11.3.1->verstack) (2.2.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.3.0->verstack) (1.4.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.3.0->verstack) (0.37.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.10.0->verstack) (1.4.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna==2.10.0->verstack) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.10.0->verstack) (21.3)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna==2.10.0->verstack) (3.13)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.2 MB/s \n",
            "\u001b[?25hCollecting alembic\n",
            "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 72.2 MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly==5.3.1->verstack) (8.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly==5.3.1->verstack) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.1->verstack) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.1->verstack) (1.1.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (3.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (13.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.14.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.6.3)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (0.24.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (4.1.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.44.0)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.3.0->holidays==0.11.3.1->verstack) (0.5.11)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0->verstack) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna==2.10.0->verstack) (3.0.8)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.10.0->verstack) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.10.0->verstack) (4.11.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->verstack) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->verstack) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->verstack) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->verstack) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==2.10.0->verstack) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->verstack) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->verstack) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->verstack) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->verstack) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->verstack) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->verstack) (3.2.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==2.10.0->verstack) (5.6.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.10.0->verstack) (3.2.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 80.8 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.1-py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 74.5 MB/s \n",
            "\u001b[?25hCollecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.10.0->verstack) (21.4.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.10.0->verstack) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==2.10.0->verstack) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->verstack) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->verstack) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->verstack) (2018.9)\n",
            "Building wheels for collected packages: verstack, pyperclip\n",
            "  Building wheel for verstack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for verstack: filename=verstack-3.0.3-py3-none-any.whl size=73100 sha256=1f910819bb7320d4bca6ade76af3bd694184879eb3d170c821056ba1e90125dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/7d/41/2b9ac43b55213e71352931fc34878f7dd9d10b887555a625f7\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=f188c12991d0035bf166843c06560b632b92a8bd2dcbdd909674a1df8738e4b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built verstack pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, python-dateutil, Mako, cmd2, autopage, tensorflow-estimator, scikit-learn, keras, gast, colorlog, cmaes, cliff, alembic, tensorflow, plotly, optuna, lightgbm, holidays, verstack\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.5.0\n",
            "    Uninstalling plotly-5.5.0:\n",
            "      Successfully uninstalled plotly-5.5.0\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Attempting uninstall: holidays\n",
            "    Found existing installation: holidays 0.10.5.2\n",
            "    Uninstalling holidays-0.10.5.2:\n",
            "      Successfully uninstalled holidays-0.10.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Mako-1.2.0 alembic-1.7.7 autopage-0.5.0 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.1 colorlog-6.6.0 gast-0.4.0 holidays-0.11.3.1 keras-2.7.0 lightgbm-3.3.0 optuna-2.10.0 pbr-5.8.1 plotly-5.3.1 pyperclip-1.8.2 python-dateutil-2.8.1 scikit-learn-1.0.1 stevedore-3.5.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 verstack-3.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.0.5-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.6 MB 107 kB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.5)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.5\n",
            "Collecting pyod\n",
            "  Downloading pyod-0.9.9.tar.gz (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyod) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pyod) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from pyod) (1.21.5)\n",
            "Requirement already satisfied: numba>=0.35 in /usr/local/lib/python3.7/dist-packages (from pyod) (0.51.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from pyod) (1.4.1)\n",
            "Requirement already satisfied: scikit_learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from pyod) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pyod) (1.15.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod) (0.10.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.35->pyod) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.35->pyod) (57.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>=0.20.0->pyod) (3.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->pyod) (4.1.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pyod) (1.3.5)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pyod) (0.5.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels->pyod) (2018.9)\n",
            "Building wheels for collected packages: pyod\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-0.9.9-py3-none-any.whl size=139325 sha256=d338aadee50175615bf7db9f71cbda78ef7bcfce694ecd03bac2352574ff5f82\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/32/f0/0dc3050775e77b6661a116b70817b02b4305fa253269d6d998\n",
            "Successfully built pyod\n",
            "Installing collected packages: pyod\n",
            "Successfully installed pyod-0.9.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "from scipy import stats\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.impute import KNNImputer\n",
        "from verstack import NaNImputer\n",
        "from pyod.models.iforest import IForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "MPJ1fGs7Qhgy",
        "pycharm": {
          "is_executing": true
        }
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # do not show warnings"
      ],
      "metadata": {
        "id": "oo0QnAcQjjZG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers"
      ],
      "metadata": {
        "id": "xpuGNAJSP_Br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model related"
      ],
      "metadata": {
        "id": "O1oIE84yslGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_vanilla(X_train, y_train, X_test, y_test):\n",
        "    ETC = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
        "    RFC = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    GBC = GradientBoostingClassifier(random_state=42)\n",
        "    LGBMC = LGBMClassifier(random_state=42, n_jobs=-1)\n",
        "    CBC = CatBoostClassifier(random_state=42, verbose=0)\n",
        "    KNN = KNeighborsClassifier(n_jobs=-1)\n",
        "    SGDC = SGDClassifier(random_state=42, n_jobs=-1)\n",
        "    MLP = MLPClassifier(random_state=42)\n",
        "\n",
        "    names = ['ETC', 'RFC', 'GBC', 'LGBMC', 'CBC', 'KNN', 'SGDC', \"MLP\"]\n",
        "    models = [ETC, RFC, GBC, LGBMC, CBC, KNN, SGDC, MLP]\n",
        "    hyperparameter_ETC = dict(n_estimators=list(range(100,180,10)),\n",
        "                              criterion=[\"gini\",\"entropy\"],\n",
        "                              max_features=[\"auto\", \"sqrt\", \"log2\"],\n",
        "                              class_weight=[\"balanced\", \"balanced_subsample\"],\n",
        "                              verbose=[0,1,2],\n",
        "                              ccp_alpha=[0,1,0.1,0.01])\n",
        "    hyperparameter_RFC = dict(n_estimators = [200,300,400], \n",
        "                              max_features = ['auto', 'sqrt'],\n",
        "                              max_depth = [int(x) for x in np.linspace(10, 120, num = 12)],\n",
        "                              min_samples_split = [2, 6, 10], \n",
        "                              min_samples_leaf = [1, 3, 4, 5, 7],\n",
        "                              bootstrap = [True, False])\n",
        "    hyperparameter_GBC = dict(min_samples_split=[100,200],\n",
        "                              min_samples_leaf=[2,4,6,12,18],\n",
        "                              max_depth=[2,4,8,16],\n",
        "                              warm_start=[True, False],\n",
        "                              validation_fraction = [0.1,0.2],\n",
        "                              ccp_alpha =[0.0, 0.2, 0.5, 0.6],\n",
        "                              max_features=\"sqrt\",\n",
        "                              n_estimators=[100,200,300,400],\n",
        "                              learning_rate = np.linspace(0,0.2,5),\n",
        "                              loss = [\"deviance\", \"exponential\"],\n",
        "                              criterion=[\"friedman_mse\", \"squared_error\", \"mse\", \"mae\"])\n",
        "    hyperparameter_LGBMC = dict(boosting_type=[\"gbdt\",\"dart\", \"goss\"],\n",
        "                                max_features=\"sqrt\",\n",
        "                                n_estimators=[100,200,300,400],\n",
        "                                learning_rate = np.linspace(0,0.2,5),\n",
        "                                reg_alpha=[0.0,0.1,0.2],\n",
        "                                reg_lambda=[0.0,0.1,0.2])\n",
        "    hyperparameter_CBC = dict(learning_rate = np.linspace(0,0.2,5),\n",
        "                              depth=np.linspace(10,50, 5))           \n",
        "    hyperparameter_KNN = dict(leaf_size=list(range(5,20)), n_neighbors=list(range(5,20)), p=[1,2])\n",
        "    hyperparameter_SGDC = dict(loss = [\"hinge\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],\n",
        "                               penalty = [\"l2\", \"l1\", \"elasticnet\"],\n",
        "                               verbose = [0,1],\n",
        "                               power_t = [0.2,0.5,0.7],\n",
        "                               early_stopping = [True],\n",
        "                               warm_start = [True])   \n",
        "    hyperparameter_MLP = dict(activation = [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
        "                              solver = [\"lbfgs\", \"sgd\", \"adam\"],\n",
        "                              learning_rate = [\"constant\", \"invscaling\", \"adaptive\"],\n",
        "                              warm_start = [True],\n",
        "                              early_stopping = [True])\n",
        "    hyperparamers = [hyperparameter_ETC, hyperparameter_RFC, hyperparameter_GBC,\n",
        "                     hyperparameter_LGBMC, hyperparameter_CBC, hyperparameter_KNN, \n",
        "                     hyperparameter_SGDC, hyperparameter_MLP]\n",
        "    for name, model, hy in zip(names, models, hyperparamers):\n",
        "        metric = 'Accuracy'\n",
        "        cv = RepeatedKFold(n_splits=5, \n",
        "                          n_repeats=1, \n",
        "                          random_state=42)\n",
        "        grid = GridSearchCV(estimator=model, \n",
        "                            param_grid={}, \n",
        "                            scoring=metric.lower(),\n",
        "                            cv=cv, \n",
        "                            verbose=0,\n",
        "                            n_jobs=-1)\n",
        "        grid.fit(X_train, y_train)\n",
        "        ypred = grid.predict(X_test)\n",
        "        result = confusion_matrix(y_test, ypred)\n",
        "        classification = classification_report(y_test, ypred)\n",
        "        print(f\"{metric} for {name}: {grid.score(X_test, y_test)}\")\n",
        "        print(f\"Best params for {name}: {grid.best_params_}\")\n",
        "        print(result)\n",
        "        print(classification)"
      ],
      "metadata": {
        "id": "HSc-4dDxsnuV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_importances(X, \n",
        "                            y, \n",
        "                            max_features: int) -> dict:\n",
        "    ETC = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
        "    RFC = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    GBC = GradientBoostingClassifier(random_state=42)\n",
        "    LGBMC = LGBMClassifier(random_state=42, n_jobs=-1)\n",
        "    CBC = CatBoostClassifier(random_state=42, verbose=0)\n",
        "    SGDC = SGDClassifier(random_state=42, n_jobs=-1)\n",
        "    MLP = MLPClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "    names = ['ETC', 'RFC', 'GBC', 'LGBMC', 'CBC', 'SGDC', \"MLP\"]\n",
        "    models = [ETC, RFC, GBC, LGBMC, CBC, SGDC, MLP]\n",
        "\n",
        "    feature_importances_0 = np.zeros((7, max_features))\n",
        "    feature_importances_1 = np.zeros((7, max_features//2))\n",
        "    feature_importances_2 = np.zeros((7, max_features//4))\n",
        "    \n",
        "    for i, name, model in zip(range(len(models)), names, models):\n",
        "        print(f'Fitting {name}...', end=' ')\n",
        "        model.fit(X, y)\n",
        "        print('Done')\n",
        "        selector_0 = SelectFromModel(model, \n",
        "                                     threshold=-np.inf,\n",
        "                                     prefit=True,\n",
        "                                     max_features=max_features)\n",
        "        feature_importances_0[i] = selector_0.get_support(indices=True)\n",
        "\n",
        "        selector_1 = SelectFromModel(model, \n",
        "                                     threshold=-np.inf,\n",
        "                                     prefit=True,\n",
        "                                     max_features=max_features//2)\n",
        "        feature_importances_1[i] = selector_1.get_support(indices=True)\n",
        "\n",
        "        selector_2 = SelectFromModel(model, \n",
        "                                     threshold=-np.inf,\n",
        "                                     prefit=True,\n",
        "                                     max_features=max_features//4)\n",
        "        feature_importances_2[i] = selector_2.get_support(indices=True)\n",
        "\n",
        "    tfi_0 = feature_importances_0.flatten()\n",
        "    tfi_1 = feature_importances_1.flatten()\n",
        "    tfi_2 = feature_importances_2.flatten()\n",
        "\n",
        "    u_0 = np.unique(tfi_0)\n",
        "    u_1 = np.unique(tfi_1)\n",
        "    u_2 = np.unique(tfi_2)\n",
        "\n",
        "    fi_dict = {f'feature_importances_{max_features}': u_0.astype(int),\n",
        "               f'feature_importances_{max_features//2}': u_1.astype(int),\n",
        "               f'feature_importances_{max_features//4}': u_2.astype(int)}\n",
        "\n",
        "    return fi_dict"
      ],
      "metadata": {
        "id": "kt2K5JTOmaT9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset class"
      ],
      "metadata": {
        "id": "Mt0E-hfuJK0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataSet:\n",
        "    \"\"\"\n",
        "    Class handling all preprocessing with different possible configurations\n",
        "\n",
        "    :param path: Path to datasets\n",
        "    :param years: List of years to be considered\n",
        "    :param nan_handler: List of NaN handling methods - Options: 'threshold', empty\n",
        "    :param zero_handler: List of zero handling methods - Options: 'threshold', 'replace', 'drop_again', empty\n",
        "                         - Is stackable, but 'drop_again' needs the 'replace' argument - e.g. ['replace', 'drop_again']\n",
        "    :param imputation_handler: Imputation method - Options: 'mean', 'KNN', 'XGB'\n",
        "    :param outlier_handler: List of outlier handling methods - Options: 'quantile', 'IForest'\n",
        "                            - Is stackable - e.g. ['quantile', 'IForest']\n",
        "    :param config_dict: Dictionary that defines the thresholds and contamination (for IForest) for each year.\n",
        "                        It has to be structured the following way:\n",
        "\n",
        "                                      config_dict = {\n",
        "                                                      2014: {\n",
        "                                                          'nans_thres': float (e.g. 0.08), \n",
        "                                                          'zeros_thres': float (e.g. 0.08),\n",
        "                                                          'cut_lower': float (e.g. 0.01),\n",
        "                                                          'cut_upper': float (e.g. 0.99),\n",
        "                                                          'IForest_contamination': float (e.g. 0.02)\n",
        "                                                      },\n",
        "                                                    ...\n",
        "                                                    }\n",
        "                        :key nans_thres: Sets the NaN threshold, such that columns that surpass this threshold\n",
        "                                         are removed (e.g. 0.08 - allowing up to 8% NaNs in columns)\n",
        "                        :key zeros_thres: Sets zero threshold, such that columns that surpass this threshold\n",
        "                                          are removed (e.g. 0.08 - allowing up to 8% zeros in columns)\n",
        "                        :key cut_lower: Sets lower bound, such that rows that have a price var which falls below \n",
        "                                        the quantile are removed (e.g. 0.01)\n",
        "                        :key cut_upper: Sets upper bound, such that rows that have a price var which is above \n",
        "                                        the quantile are removed (e.g. 0.01)\n",
        "                        : key IForest_contamination: Sets the contamination percentage of the dataset (e.g 0.02 - 2% outliers)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 path: str = '',\n",
        "                 benchmark = 2.5,\n",
        "                 counter = 0,\n",
        "                 sp_index: list = [-0.73, 9.54, 19.42, -6.24, 28.88],\n",
        "                 years: list = [2014, 2015, 2016, 2017, 2018],\n",
        "                 nan_handler: list = ['threshold'],\n",
        "                 zero_handler: list = ['threshold', 'replace'],\n",
        "                 imputation_handler: str = 'XGB',\n",
        "                 outlier_handler: list = ['quantile', 'IForest'],\n",
        "                 config_dict: dict = {}\n",
        "                 ):\n",
        "        self.path = path\n",
        "        self.years = years\n",
        "        self.nan_handler = nan_handler\n",
        "        self.zero_handler = zero_handler\n",
        "        self.imputation_handler = imputation_handler\n",
        "        self.outlier_handler = outlier_handler\n",
        "        self.config_dict = config_dict\n",
        "        self.sp_index = sp_index\n",
        "        self.benchmark = benchmark\n",
        "        self.counter = counter\n",
        "\n",
        "        self.df = None\n",
        "        self.year = None\n",
        "        self.init_shape = None\n",
        "        self.class_col = None\n",
        "        self.sector_col = None\n",
        "        self.sector_ids = None\n",
        "        self.sector_mapper = None\n",
        "        self.dfs = []\n",
        "        self.intersec_cols = None\n",
        "\n",
        "    def create_dataset(self):\n",
        "        \"\"\"\n",
        "        Creates the whole dataset:\n",
        "            - Stores dataframe per year in self.dfs\n",
        "            - Finds intersecting columns between all dataframes\n",
        "            - Concatenates all dataframes with the selected columns\n",
        "            - Factorize the 'Sector' column -> Assign ID to each string\n",
        "            - Rename feature names to avoid UTF-8 encoding issues with LGBM\n",
        "        \"\"\"\n",
        "\n",
        "        # loop over all years\n",
        "        for year in self.years:\n",
        "            self.__print_sep(60, '#', '\\n')\n",
        "\n",
        "            # generate single dataframe\n",
        "            self.prepare_single_dataframe(year)\n",
        "\n",
        "            # append dataframe\n",
        "            self.dfs.append(self.df)\n",
        "            self.counter += 1\n",
        "\n",
        "        # print seperators\n",
        "        self.__print_sep(60, '#')\n",
        "        self.__print_sep(60, '#', '\\n')\n",
        "\n",
        "        # get intersecting columns\n",
        "        self.get_intersecting_columns()\n",
        "\n",
        "        # concatenate dataframes to one\n",
        "        df = self.concat_intersecting_dfs()\n",
        "\n",
        "        # factorize 'Sector' column\n",
        "        df = self.factorize_col(df)\n",
        "\n",
        "        # avoid encoding issues with feature names\n",
        "        df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_single_dataframe(self, year: int):\n",
        "        \"\"\"\n",
        "        Prepares a dataframe for a specific year\n",
        "\n",
        "        :param year: Sets year\n",
        "\n",
        "            - Loads data into dataframe\n",
        "            - Drop the 'Class' column for preprocessing\n",
        "            - Preprocess NaNs\n",
        "            - Preprocess zeros\n",
        "            - Factorize 'Sector' column for imputation\n",
        "            - Impute NaNs\n",
        "            - Detect and remove outliers\n",
        "            - Scale data\n",
        "            - Convert data to float32 and join back 'Class' column\n",
        "            - Map back the 'Sector' column\n",
        "        \"\"\"\n",
        "        # load data\n",
        "        self.load_df(year)\n",
        "\n",
        "        # store and drop 'Class' column\n",
        "        self.store_drop_class_col()\n",
        "\n",
        "        # handle NaNs\n",
        "        self.handle_nans()\n",
        "\n",
        "        # handle zeros\n",
        "        self.handle_zeros()\n",
        "\n",
        "        # factorize 'Sector' column and store its mapper\n",
        "        self.store_factorize_sector_col()\n",
        "\n",
        "        # impute NaNs\n",
        "        self.impute_nans()\n",
        "\n",
        "        # handle outliers\n",
        "        self.handle_outliers()\n",
        "\n",
        "        # drop VAR column\n",
        "        self.drop_var_col()\n",
        "\n",
        "        # scale data\n",
        "        self.scale_data()\n",
        "\n",
        "        # convert to float 32 and join\n",
        "        self.convert_join()\n",
        "\n",
        "        # map 'Sector' back to string representation\n",
        "        self.map_sector_inv()\n",
        "\n",
        "    def load_df(self, year: int):\n",
        "        self.__print_header(f'LOADING {year}')\n",
        "        self.year = year\n",
        "        print(f'Loading {year}_Financial_Data.csv into a DataFrame', end=' - ')\n",
        "        self.df = pd.read_csv(f'/content/{year}_Financial_Data.csv',\n",
        "                              index_col=0)\n",
        "        print('COMPLETE')\n",
        "        self.init_shape = self.df.shape\n",
        "        # label the classes for our strategy with 0 = sell, 1 = hold, 2 = buy if it performs better, \n",
        "        # less or between benchmark = 2.5%\n",
        "        buy = self.sp_index[self.counter] + self.benchmark\n",
        "        sell = self.sp_index[self.counter] - self.benchmark\n",
        "        self.df.loc[self.df[f'{year+1} PRICE VAR [%]'] > buy, \"Class\"] = 2\n",
        "        self.df.loc[self.df[f'{year+1} PRICE VAR [%]'] < sell, \"Class\"] = 0\n",
        "        self.df.loc[(self.df[f'{year+1} PRICE VAR [%]'] >= sell) & (self.df[f'{year+1} PRICE VAR [%]'] <= buy), \"Class\"] = 1\n",
        "        # drop rows with no info\n",
        "        print('Dropping rows with NaNs only', end=' - ')\n",
        "        rows = self.df.shape[0]\n",
        "        self.df.dropna(how='all', inplace=True)\n",
        "        print(f'{self.df.shape[0] - rows} rows dropped - COMPLETE')\n",
        "\n",
        "        self.__print_summary(f'Initial DataFrame shape: {self.df.shape}')\n",
        "\n",
        "    def store_drop_class_col(self):\n",
        "        self.class_col = self.df['Class'].astype('int8')\n",
        "        self.df.drop(columns=['Class'], inplace=True)\n",
        "\n",
        "    def store_factorize_sector_col(self):\n",
        "        self.sector_col = self.df['Sector']\n",
        "        self.sector_ids, self.sector_mapper = pd.factorize(self.df['Sector'])\n",
        "        self.df['Sector'] = self.sector_ids\n",
        "\n",
        "    def factorize_col(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        sector_ids, self.sector_mapper = pd.factorize(df['Sector'])\n",
        "        df['Sector'] = sector_ids\n",
        "        df['Sector'] = df['Sector'].astype('float32')\n",
        "        return df\n",
        "\n",
        "    def handle_nans(self, round: int = 1):\n",
        "        if round == 1 and self.nan_handler:\n",
        "            self.__print_header(f'HANDLING NANS')\n",
        "\n",
        "        if 'threshold' in self.nan_handler:\n",
        "            nans_thres = self.config_dict[self.year]['nans_thres']\n",
        "            print(f'Dropping columns with more than {int(nans_thres * 100)}% NaNs', end=' - ')\n",
        "            cols = self.df.shape[1]\n",
        "            self.df = self.df.loc[:, self.df.isnull().mean() < nans_thres]  # drop nans\n",
        "            print(f'{cols - self.df.shape[1]} columns dropped - COMPLETE')\n",
        "            self.__print_sep(60, '~', '\\n')\n",
        "\n",
        "    def handle_zeros(self):\n",
        "        self.__print_header(f'HANDLING ZEROS')\n",
        "\n",
        "        if 'threshold' in self.zero_handler:\n",
        "            zeros_thres = self.config_dict[self.year]['zeros_thres']\n",
        "            print(f'Dropping columns with more than {int(zeros_thres * 100)}% zeros', end=' - ')\n",
        "            cols = self.df.shape[1]\n",
        "            self.df = self.df.loc[:, (self.df == 0).mean() < zeros_thres]  # drop zeros\n",
        "            print(f'{cols - self.df.shape[1]} columns dropped - COMPLETE')\n",
        "\n",
        "        if 'replace' in self.zero_handler:\n",
        "            print(f'Replacing zeros with NaNs', end=' - ')\n",
        "            z_count = (self.df == 0).sum().sum()\n",
        "            self.df = self.df.replace({0: np.nan})\n",
        "            z_count_after = (self.df == 0).sum().sum()\n",
        "            print(f'{z_count - z_count_after} zeros replaced - COMPLETE')\n",
        "\n",
        "        if 'replace' in self.zero_handler and 'drop_again' in self.zero_handler:\n",
        "            self.handle_nans(round=2)\n",
        "\n",
        "        da = self.init_shape[1] - self.df.shape[1]\n",
        "        sc = self.df.shape\n",
        "        self.__print_summary(f'Total amount of columns dropped: {da} - Current shape: {sc}')\n",
        "\n",
        "    def impute_nans(self):\n",
        "        self.__print_header(f'IMPUTE NANS')\n",
        "\n",
        "        if self.imputation_handler == 'KNN':\n",
        "            print(f'Imputing {self.df.isnull().sum().sum()} NaNs with KNN', end=' - ')\n",
        "            imputer = KNNImputer(n_neighbors=20, weights='distance', metric='nan_euclidean', copy=True)\n",
        "            np_imp = imputer.fit_transform(self.df)\n",
        "            self.df = pd.DataFrame(np_imp, columns=self.df.columns, index=self.df.index)\n",
        "            print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
        "\n",
        "        elif self.imputation_handler == 'XGB':\n",
        "            print(f'Imputing {self.df.isnull().sum().sum()} NaNs with XGB', end=' - ')\n",
        "            imputer = NaNImputer(verbose=False, multiprocessing_load=1)\n",
        "            self.df = imputer.impute(self.df)\n",
        "            print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
        "            cols = self.df.shape[1]\n",
        "            self.df.dropna(axis=1, inplace=True)  # remove columns that still have NaNs inside\n",
        "            print(f'Removing {cols - self.df.shape[1]} columns such that', end=' ')\n",
        "            print(f'{self.df.isnull().sum().sum()} NaNs are left - COMPLETE')\n",
        "\n",
        "        elif self.imputation_handler == 'mean':\n",
        "            print(f'Imputing NaNs with mean')\n",
        "\n",
        "            # drop rows where the 'Revenue' is unknown (NaNs)\n",
        "            print(f'Dropping rows that have no information about revenue', end=' - ')\n",
        "            rows = self.df.shape[0]\n",
        "            self.df.drop(self.df[self.df['Revenue'].isnull()].index, inplace=True)\n",
        "            print(f'{rows - self.df.shape[0]} rows dropped - COMPLETE')\n",
        "\n",
        "            # introduce new column that describes revenue ranges\n",
        "            range_cond = [(self.df['Revenue'] <= 1e6),\n",
        "                          (self.df['Revenue'] > 1e6) & (self.df['Revenue'] <= 1e7),\n",
        "                          (self.df['Revenue'] > 1e7) & (self.df['Revenue'] <= 1e8),\n",
        "                          (self.df['Revenue'] > 1e8) & (self.df['Revenue'] <= 1e9),\n",
        "                          (self.df['Revenue'] > 1e9)]\n",
        "            self.df['Revenue Range'] = np.select(range_cond, [0, 1, 2, 3, 4])\n",
        "\n",
        "            # store sector column to add it after groupby\n",
        "            sector_col = self.df['Sector']\n",
        "\n",
        "            # group by sector and then revenue range and impute nans with means\n",
        "            nans_count = self.df.isnull().sum().sum()\n",
        "            self.df = self.df.groupby(['Sector', 'Revenue Range']).transform(lambda x: x.fillna(x.mean()))\n",
        "            nans_count_after = self.df.isnull().sum().sum()\n",
        "            print(f'{nans_count - nans_count_after} NaNs imputed', end=' - ')\n",
        "            print(f'{nans_count_after} NaNs left - COMPLETE')\n",
        "\n",
        "            # add sector column again\n",
        "            self.df['Sector'] = sector_col\n",
        "\n",
        "            # if there are still NaNs left, remove the corresponding row(s)\n",
        "            if nans_count_after > 0:\n",
        "                print(f'Dropping rows with remaining NaNs', end=' - ')\n",
        "                row_count = self.df.shape[0]\n",
        "                self.df = self.df.dropna(axis=0)\n",
        "                print(f'{row_count - self.df.shape[0]} row(s) dropped', end=' - ')\n",
        "                print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
        "\n",
        "        self.__print_sep(60, '~', '\\n')\n",
        "\n",
        "    def handle_outliers(self):\n",
        "        self.__print_header('HANDLE OUTLIERS')\n",
        "\n",
        "        if 'quantile' in self.outlier_handler:\n",
        "            rows_before = self.df.shape[0]\n",
        "            cut_lower = self.config_dict[self.year]['cut_lower']\n",
        "            cut_upper = self.config_dict[self.year]['cut_upper']\n",
        "            # drop rows with an unnaturally high price variance\n",
        "            print(f'Dropping rows with a price variance outside the {cut_lower} - {cut_upper} quantile range',\n",
        "                  end=' - ')\n",
        "            col = f'{self.year + 1} PRICE VAR [%]'\n",
        "            outs = self.df[col].between(self.df[col].quantile(cut_lower),\n",
        "                                        self.df[col].quantile(cut_upper))\n",
        "            self.df.drop(self.df[~outs].index, inplace=True)\n",
        "            print(f'{rows_before - self.df.shape[0]} rows dropped - COMPLETE')\n",
        "\n",
        "        if 'IForest' in self.outlier_handler:\n",
        "            print('Using Isolation Forest to detect outliers', end=' - ')\n",
        "            contamination = self.config_dict[self.year]['IForest_contamination']\n",
        "            clf = IForest(contamination=contamination,\n",
        "                          random_state=42,\n",
        "                          n_jobs=-1)\n",
        "            clf.fit(self.df.values)\n",
        "            y_pred = clf.predict(self.df.values)\n",
        "            idx_y_pred = [i for i in range(self.df.shape[0]) if y_pred[i] == 0]\n",
        "            self.df = self.df.iloc[idx_y_pred, :]\n",
        "            print(f'{sum(y_pred)} outliers removed - COMPLETE')\n",
        "\n",
        "        self.__print_sep(60, '~', '\\n')\n",
        "\n",
        "    def scale_data(self):\n",
        "        self.__print_header('SCALING DATA')\n",
        "\n",
        "        print(f'Scaling data', end=' - ')\n",
        "        sector_col = self.df['Sector'] if 'Sector' in self.df.columns else None\n",
        "        scaler = StandardScaler()\n",
        "        np_scaled = scaler.fit_transform(self.df)\n",
        "        self.df = pd.DataFrame(np_scaled,\n",
        "                               columns=self.df.columns,\n",
        "                               index=self.df.index)\n",
        "\n",
        "        if sector_col is not None:\n",
        "            self.df['Sector'] = sector_col\n",
        "        print('COMPLETE')\n",
        "\n",
        "    def drop_var_col(self):\n",
        "        self.df.drop(columns=[f'{self.year + 1} PRICE VAR [%]'], inplace=True)\n",
        "\n",
        "    def convert_join(self):\n",
        "        # convert to float32\n",
        "        self.df = self.df.astype('float32')\n",
        "\n",
        "        # join 'Class' column\n",
        "        self.df = self.df.join(self.class_col)\n",
        "\n",
        "        self.__print_summary(f'Final DataFrame shape: {self.df.shape}')\n",
        "\n",
        "    def map_sector_inv(self):\n",
        "        mapper = {i: sector for i, sector in enumerate(self.sector_mapper)}\n",
        "        self.df['Sector'] = self.df['Sector'].apply(lambda x: mapper[x])\n",
        "\n",
        "    def get_intersecting_columns(self):\n",
        "        df1 = self.dfs[0].columns\n",
        "        df2 = self.dfs[1].columns\n",
        "        df3 = self.dfs[2].columns\n",
        "        df4 = self.dfs[3].columns\n",
        "        df5 = self.dfs[4].columns\n",
        "\n",
        "        self.intersec_cols = df1 & df2 & df3 & df4 & df5\n",
        "        print(f'Found {len(self.intersec_cols)} intersecting columns!')\n",
        "\n",
        "    def concat_intersecting_dfs(self):\n",
        "        df1 = self.dfs[0][self.intersec_cols]\n",
        "        df2 = self.dfs[1][self.intersec_cols]\n",
        "        df3 = self.dfs[2][self.intersec_cols]\n",
        "        df4 = self.dfs[3][self.intersec_cols]\n",
        "        df5 = self.dfs[4][self.intersec_cols]\n",
        "\n",
        "        df = pd.concat([df1, df2, df3, df4, df5])\n",
        "        print(f'Concatenated DataFrame into shape: {df.shape}')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def __print_summary(self, info: str):\n",
        "        self.__print_sep()\n",
        "        print(info)\n",
        "        self.__print_sep(60, '~', '\\n')\n",
        "\n",
        "    def __print_sep(self, n: int = 60, c: str = '-', nl: str = ''):\n",
        "        print(n * c + nl)\n",
        "\n",
        "    def __print_header(self, header: str):\n",
        "        rem = 60 - 25 - 2 - len(header)\n",
        "        print(f'{25 * \"~\"} {header} {rem * \"~\"}')"
      ],
      "metadata": {
        "id": "Qb_W1JWaj498"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare approaches"
      ],
      "metadata": {
        "id": "HNHa5JNemgs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set config dictionary"
      ],
      "metadata": {
        "id": "KhDP7N_Bs9Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_dict = {\n",
        "    2014: {\n",
        "        'nans_thres': 0.08, \n",
        "        'zeros_thres': 0.08,\n",
        "        'cut_lower': 0.01,\n",
        "        'cut_upper': 0.99,\n",
        "        'IForest_contamination': 0.02\n",
        "    },\n",
        "    2015: {\n",
        "        'nans_thres': 0.10, \n",
        "        'zeros_thres': 0.08,\n",
        "        'cut_lower': 0.01,\n",
        "        'cut_upper': 0.99,\n",
        "        'IForest_contamination': 0.02\n",
        "    },\n",
        "    2016: {\n",
        "        'nans_thres': 0.16, \n",
        "        'zeros_thres': 0.08,\n",
        "        'cut_lower': 0.01,\n",
        "        'cut_upper': 0.99,\n",
        "        'IForest_contamination': 0.02\n",
        "    },\n",
        "    2017: {\n",
        "        'nans_thres': 0.16, \n",
        "        'zeros_thres': 0.08,\n",
        "        'cut_lower': 0.01,\n",
        "        'cut_upper': 0.99,\n",
        "        'IForest_contamination': 0.02\n",
        "    },\n",
        "    2018: {\n",
        "        'nans_thres': 0.08, \n",
        "        'zeros_thres': 0.08,\n",
        "        'cut_lower': 0.01,\n",
        "        'cut_upper': 0.99,\n",
        "        'IForest_contamination': 0.02\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "4OwwwrQwgiO-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 1\n",
        "\n",
        "1.   *NaN threshold*\n",
        "2.   *zero threshold*\n",
        "3.   *mean imputation*\n",
        "4.   *outlier quantile cut*\n",
        "\n"
      ],
      "metadata": {
        "id": "otz1ng9emkbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Lni6vyS2hqx6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_1 = DataSet(config_dict=config_dict,\n",
        "                    nan_handler = ['threshold'],\n",
        "                    zero_handler = ['threshold'],\n",
        "                    imputation_handler = 'mean',\n",
        "                    outlier_handler = ['quantile'])\n",
        "\n",
        "# generate DataFrame\n",
        "df_1 = dataset_1.create_dataset()\n",
        "\n",
        "# use full df for cross validation vanilla test\n",
        "X_1, y_1 = df_1.drop(columns=['Class']), df_1['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1, test_size=0.2, random_state = 42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyMm5zQ8gZnV",
        "outputId": "71542ba8-b9fe-4f5d-92fd-f226f94ce970"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2014 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2014_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (3808, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% NaNs - 89 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 60 columns dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 150 - Current shape: (3808, 74)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing NaNs with mean\n",
            "Dropping rows that have no information about revenue - 44 rows dropped - COMPLETE\n",
            "8163 NaNs imputed - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 76 rows dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3688, 74)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2015 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2015_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4120, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 10% NaNs - 68 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 67 columns dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 136 - Current shape: (4120, 88)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing NaNs with mean\n",
            "Dropping rows that have no information about revenue - 67 rows dropped - COMPLETE\n",
            "16665 NaNs imputed - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 82 rows dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3971, 88)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2016 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2016_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4797, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 16% NaNs - 97 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 61 columns dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 159 - Current shape: (4797, 65)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing NaNs with mean\n",
            "Dropping rows that have no information about revenue - 489 rows dropped - COMPLETE\n",
            "9768 NaNs imputed - 10 NaNs left - COMPLETE\n",
            "Dropping rows with remaining NaNs - 1 row(s) dropped - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 88 rows dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4219, 65)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2017 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2017_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4960, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 16% NaNs - 145 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 31 columns dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 177 - Current shape: (4960, 47)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing NaNs with mean\n",
            "Dropping rows that have no information about revenue - 525 rows dropped - COMPLETE\n",
            "5506 NaNs imputed - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 90 rows dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4345, 47)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2018 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2018_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4392, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% NaNs - 62 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 70 columns dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 133 - Current shape: (4392, 91)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing NaNs with mean\n",
            "Dropping rows that have no information about revenue - 46 rows dropped - COMPLETE\n",
            "14056 NaNs imputed - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 88 rows dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4258, 91)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "############################################################\n",
            "\n",
            "Found 42 intersecting columns!\n",
            "Concatenated DataFrame into shape: (20481, 42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vanilla(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw4By1i5oz-e",
        "outputId": "da01a1eb-4820-43e2-bc90-5731d83272b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for ETC: 0.6197217476202099\n",
            "Best params for ETC: {}\n",
            "[[1738   12  422]\n",
            " [ 151    6   91]\n",
            " [ 877    5  795]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.80      0.70      2172\n",
            "           1       0.26      0.02      0.04       248\n",
            "           2       0.61      0.47      0.53      1677\n",
            "\n",
            "    accuracy                           0.62      4097\n",
            "   macro avg       0.50      0.43      0.43      4097\n",
            "weighted avg       0.60      0.62      0.59      4097\n",
            "\n",
            "Accuracy for RFC: 0.6204539907249207\n",
            "Best params for RFC: {}\n",
            "[[1818    4  350]\n",
            " [ 153    4   91]\n",
            " [ 956    1  720]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.84      0.71      2172\n",
            "           1       0.44      0.02      0.03       248\n",
            "           2       0.62      0.43      0.51      1677\n",
            "\n",
            "    accuracy                           0.62      4097\n",
            "   macro avg       0.56      0.43      0.42      4097\n",
            "weighted avg       0.61      0.62      0.59      4097\n",
            "\n",
            "Accuracy for GBC: 0.6023919941420551\n",
            "Best params for GBC: {}\n",
            "[[1864    3  305]\n",
            " [ 172    1   75]\n",
            " [1070    4  603]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.86      0.71      2172\n",
            "           1       0.12      0.00      0.01       248\n",
            "           2       0.61      0.36      0.45      1677\n",
            "\n",
            "    accuracy                           0.60      4097\n",
            "   macro avg       0.45      0.41      0.39      4097\n",
            "weighted avg       0.58      0.60      0.56      4097\n",
            "\n",
            "Accuracy for LGBMC: 0.6194776665853063\n",
            "Best params for LGBMC: {}\n",
            "[[1804    6  362]\n",
            " [ 153    3   92]\n",
            " [ 943    3  731]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.83      0.71      2172\n",
            "           1       0.25      0.01      0.02       248\n",
            "           2       0.62      0.44      0.51      1677\n",
            "\n",
            "    accuracy                           0.62      4097\n",
            "   macro avg       0.50      0.43      0.42      4097\n",
            "weighted avg       0.60      0.62      0.59      4097\n",
            "\n",
            "Accuracy for CBC: 0.6202099096900171\n",
            "Best params for CBC: {}\n",
            "[[1763    3  406]\n",
            " [ 145    4   99]\n",
            " [ 902    1  774]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.81      0.71      2172\n",
            "           1       0.50      0.02      0.03       248\n",
            "           2       0.61      0.46      0.52      1677\n",
            "\n",
            "    accuracy                           0.62      4097\n",
            "   macro avg       0.58      0.43      0.42      4097\n",
            "weighted avg       0.61      0.62      0.59      4097\n",
            "\n",
            "Accuracy for KNN: 0.5660239199414205\n",
            "Best params for KNN: {}\n",
            "[[1582   35  555]\n",
            " [ 158   14   76]\n",
            " [ 916   38  723]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.73      0.66      2172\n",
            "           1       0.16      0.06      0.08       248\n",
            "           2       0.53      0.43      0.48      1677\n",
            "\n",
            "    accuracy                           0.57      4097\n",
            "   macro avg       0.43      0.41      0.41      4097\n",
            "weighted avg       0.54      0.57      0.55      4097\n",
            "\n",
            "Accuracy for SGDC: 0.4659506956309495\n",
            "Best params for SGDC: {}\n",
            "[[ 789   45 1338]\n",
            " [  56    8  184]\n",
            " [ 523   42 1112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.36      0.45      2172\n",
            "           1       0.08      0.03      0.05       248\n",
            "           2       0.42      0.66      0.52      1677\n",
            "\n",
            "    accuracy                           0.47      4097\n",
            "   macro avg       0.36      0.35      0.34      4097\n",
            "weighted avg       0.48      0.47      0.45      4097\n",
            "\n",
            "Accuracy for MLP: 0.5855504027337076\n",
            "Best params for MLP: {}\n",
            "[[1794    2  376]\n",
            " [ 179    0   69]\n",
            " [1070    2  605]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.83      0.69      2172\n",
            "           1       0.00      0.00      0.00       248\n",
            "           2       0.58      0.36      0.44      1677\n",
            "\n",
            "    accuracy                           0.59      4097\n",
            "   macro avg       0.39      0.40      0.38      4097\n",
            "weighted avg       0.55      0.59      0.55      4097\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 2\n",
        "\n",
        "\n",
        "1.   *zero threshold*\n",
        "2.   *replace zeros with NaNs*\n",
        "3.   *KNN imputation*\n",
        "4.   *outlier quantile cut*\n",
        "5.   *Isolation Forest outlier handling*\n",
        "\n"
      ],
      "metadata": {
        "id": "H4SGEHkIo7kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_2 = DataSet(config_dict=config_dict,\n",
        "                    nan_handler = [],\n",
        "                    zero_handler = ['threshold', 'replace'],\n",
        "                    imputation_handler = 'KNN',\n",
        "                    outlier_handler = ['quantile', 'IForest'])\n",
        "\n",
        "# generate DataFrame\n",
        "df_2 = dataset_2.create_dataset()\n",
        "\n",
        "# use full df for cross validation vanilla test\n",
        "X_2, y_2 = df_2.drop(columns=['Class']), df_2['Class']\n",
        "\n",
        "# split into train test\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_2, y_2, test_size=0.2, random_state = 42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KNjNTA_PqSil",
        "outputId": "8ba76017-a7f3-429f-92e3-75f8022911b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2014 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2014_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (3808, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 87 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 7842 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 88 - Current shape: (3808, 136)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 77059 NaNs with KNN - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 78 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 75 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3655, 136)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2015 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2015_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4120, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 84 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 8989 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 85 - Current shape: (4120, 139)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 87684 NaNs with KNN - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 84 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a553cb4c5626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# generate DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# use full df for cross validation vanilla test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-df4dccd33590>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# generate single dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_single_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# append dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-df4dccd33590>\u001b[0m in \u001b[0;36mprepare_single_dataframe\u001b[0;34m(self, year)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# handle outliers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# drop VAR column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-df4dccd33590>\u001b[0m in \u001b[0;36mhandle_outliers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m                           \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                           n_jobs=-1)\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0midx_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyod/models/iforest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    233\u001b[0m                                              verbose=self.verbose)\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# invert decision_scores_. Outliers comes with higher outlier scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_iforest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         super()._fit(\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         )\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             )\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         )\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vanilla(X_train2, y_train2, X_test2, y_test2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKZb3loZrFCr",
        "outputId": "6028c62d-afcd-477d-c052-ce80e2c702bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for ETC: 0.6426886792452831\n",
            "Best params for ETC: {}\n",
            "[[1875    6  395]\n",
            " [ 173   37   85]\n",
            " [ 847    9  813]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.82      0.73      2276\n",
            "           1       0.71      0.13      0.21       295\n",
            "           2       0.63      0.49      0.55      1669\n",
            "\n",
            "    accuracy                           0.64      4240\n",
            "   macro avg       0.66      0.48      0.50      4240\n",
            "weighted avg       0.64      0.64      0.62      4240\n",
            "\n",
            "Accuracy for RFC: 0.6438679245283019\n",
            "Best params for RFC: {}\n",
            "[[1900    2  374]\n",
            " [ 178   33   84]\n",
            " [ 871    1  797]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.83      0.73      2276\n",
            "           1       0.92      0.11      0.20       295\n",
            "           2       0.64      0.48      0.55      1669\n",
            "\n",
            "    accuracy                           0.64      4240\n",
            "   macro avg       0.73      0.47      0.49      4240\n",
            "weighted avg       0.66      0.64      0.62      4240\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 3\n",
        "\n",
        "1.   *replace zeros with NaNs*\n",
        "3.   *XGB imputation*\n",
        "4.   *outlier quantile cut*\n",
        "5.   *Isolation Forest outlier handling*\n"
      ],
      "metadata": {
        "id": "cKkOx65LrH0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_3 = DataSet(config_dict=config_dict,\n",
        "                    nan_handler = [],\n",
        "                    zero_handler = ['replace'],\n",
        "                    imputation_handler = 'XGB',\n",
        "                    outlier_handler = ['quantile', 'IForest'])\n",
        "\n",
        "# generate DataFrame\n",
        "df_3 = dataset_3.create_dataset()\n",
        "\n",
        "# use full df for cross validation vanilla test\n",
        "X_3, y_3= df_3.drop(columns=['Class']), df_3['Class']\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_3, y_3, test_size=0.2, random_state = 42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-dLVG_8rceJ",
        "outputId": "83318597-1593-4603-cddd-726dab59d4f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2014 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2014_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (3808, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Replacing zeros with NaNs - 113537 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 1 - Current shape: (3808, 223)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 214640 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "824 NaNs left - COMPLETE\n",
            "Removing 3 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 78 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 75 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3655, 217)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2015 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2015_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4120, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Replacing zeros with NaNs - 120324 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 1 - Current shape: (4120, 223)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 234672 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "5825 NaNs left - COMPLETE\n",
            "Removing 5 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 84 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 81 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3955, 217)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2016 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2016_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4797, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Replacing zeros with NaNs - 130018 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 1 - Current shape: (4797, 223)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 342567 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "3577 NaNs left - COMPLETE\n",
            "Removing 4 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 96 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 94 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4607, 218)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2017 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2017_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4960, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Replacing zeros with NaNs - 132119 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 1 - Current shape: (4960, 223)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 358388 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "0 NaNs left - COMPLETE\n",
            "Removing 0 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 100 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 98 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4762, 222)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2018 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2018_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4392, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Replacing zeros with NaNs - 131782 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 1 - Current shape: (4392, 223)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 229080 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "0 NaNs left - COMPLETE\n",
            "Removing 0 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 88 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 87 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4217, 222)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "############################################################\n",
            "\n",
            "Found 215 intersecting columns!\n",
            "Concatenated DataFrame into shape: (21196, 215)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vanilla(X_train3, y_train3, X_test3, y_test3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "uOvsf0o5ro21",
        "outputId": "53929373-2d0c-455d-885b-8991020f2d30"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for ETC: 0.6224056603773584\n",
            "Best params for ETC: {}\n",
            "[[1840    6  437]\n",
            " [ 193    5   96]\n",
            " [ 863    6  794]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.81      0.71      2283\n",
            "           1       0.29      0.02      0.03       294\n",
            "           2       0.60      0.48      0.53      1663\n",
            "\n",
            "    accuracy                           0.62      4240\n",
            "   macro avg       0.51      0.43      0.42      4240\n",
            "weighted avg       0.60      0.62      0.59      4240\n",
            "\n",
            "Accuracy for RFC: 0.6233490566037736\n",
            "Best params for RFC: {}\n",
            "[[1888    4  391]\n",
            " [ 200    3   91]\n",
            " [ 906    5  752]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.83      0.72      2283\n",
            "           1       0.25      0.01      0.02       294\n",
            "           2       0.61      0.45      0.52      1663\n",
            "\n",
            "    accuracy                           0.62      4240\n",
            "   macro avg       0.50      0.43      0.42      4240\n",
            "weighted avg       0.60      0.62      0.59      4240\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-52f56c174e4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_vanilla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-03c84843d115>\u001b[0m in \u001b[0;36mtest_vanilla\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     68\u001b[0m                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                             n_jobs=-1)\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0msample_weight_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mbegin_at_stage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0mmonitor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         )\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mX_csc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mX_csr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         )\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 4\n",
        "\n",
        "1.   *NaN threshold*\n",
        "2.   *zero threshold*\n",
        "3.   *replace zeros with NaNs*\n",
        "4.   *NaN threshold again*\n",
        "5.   *KNN imputation*\n",
        "6.   *outlier quantile cut*\n",
        "7.   *Isolation Forest outlier handling*"
      ],
      "metadata": {
        "id": "Khb0Dm11rpyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_4 = DataSet(config_dict=config_dict,\n",
        "                    nan_handler = ['threshold'],\n",
        "                    zero_handler = ['threshold', 'replace', 'drop_again'],\n",
        "                    imputation_handler = 'XGB',\n",
        "                    outlier_handler = ['quantile', 'IForest'])\n",
        "\n",
        "# generate DataFrame\n",
        "df_4 = dataset_4.create_dataset()\n",
        "\n",
        "# use full df for cross validation vanilla test\n",
        "X_4, y_4= df_4.drop(columns=['Class']), df_4['Class']\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X_4, y_4, test_size=0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "9STn0QbfsV6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_vanilla(X_train4, y_train4, X_test4, y_test4)"
      ],
      "metadata": {
        "id": "Ay4t_2VOsf8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k4vshOXtEZO6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}