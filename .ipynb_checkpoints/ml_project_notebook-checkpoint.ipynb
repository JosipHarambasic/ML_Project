{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3H2YSwnoPX6y",
    "outputId": "f45acd77-7f08-4884-82d4-43b44d6e82f2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhX8N-rzP9Rr"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NNoo1k3ZmBBO",
    "outputId": "f0dfd4f2-c2fa-4485-b0c0-162b6ad0aebc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MPJ1fGs7Qhgy"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/josip/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: /Users/josip/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7k/9x94hbs904jgh5ymqv00n1rh0000gn/T/ipykernel_32926/1689198235.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCVBooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCFUNCTYPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/ctypes/__init__.py\u001b[0m in \u001b[0;36mLoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dlltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0m__class_getitem__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenericAlias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/josip/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: /Users/josip/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from verstack import NaNImputer\n",
    "from pyod.models.iforest import IForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oo0QnAcQjjZG"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # do not show warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpuGNAJSP_Br"
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1oIE84yslGV"
   },
   "source": [
    "## Model related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSc-4dDxsnuV"
   },
   "outputs": [],
   "source": [
    "def test_vanilla(X, y):\n",
    "    ETC = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
    "    RFC = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    GBC = GradientBoostingClassifier(random_state=42)\n",
    "    LGBMC = LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "    #XGBC = XGBClassifier(seed=42)\n",
    "    CBC = CatBoostClassifier(random_state=42, verbose=0)\n",
    "    KNN = KNeighborsClassifier(n_jobs=-1)\n",
    "    SGDC = SGDClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "    names = ['ETC', 'RFC', 'GBC', 'LGBMC', 'CBC', 'KNN', 'SGDC']\n",
    "    models = [ETC, RFC, GBC, LGBMC, CBC, KNN, SGDC]\n",
    "\n",
    "    for name, model in zip(names, models):\n",
    "        metric = 'Accuracy'\n",
    "        cv = RepeatedKFold(n_splits=5, \n",
    "                          n_repeats=1, \n",
    "                          random_state=42)\n",
    "        grid = GridSearchCV(estimator=model, \n",
    "                            param_grid={}, \n",
    "                            scoring=metric.lower(),\n",
    "                            cv=cv, \n",
    "                            verbose=0,\n",
    "                            n_jobs=-1)\n",
    "        grid.fit(X, y)\n",
    "        print(f\"{metric} for {name}: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt2K5JTOmaT9"
   },
   "outputs": [],
   "source": [
    "def get_feature_importances(X, \n",
    "                            y, \n",
    "                            max_features: int) -> dict:\n",
    "    ETC = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
    "    RFC = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    GBC = GradientBoostingClassifier(random_state=42)\n",
    "    LGBMC = LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "    #XGBC = XGBClassifier(seed=42)\n",
    "    CBC = CatBoostClassifier(random_state=42, verbose=0)\n",
    "    SGDC = SGDClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "    names = ['ETC', 'RFC', 'GBC', 'LGBMC', 'CBC', 'SGDC']\n",
    "    models = [ETC, RFC, GBC, LGBMC, CBC, SGDC]\n",
    "\n",
    "    feature_importances_0 = np.zeros((7, max_features))\n",
    "    feature_importances_1 = np.zeros((7, max_features//2))\n",
    "    feature_importances_2 = np.zeros((7, max_features//4))\n",
    "    \n",
    "    for i, name, model in zip(range(len(models)), names, models):\n",
    "        print(f'Fitting {name}...', end=' ')\n",
    "        model.fit(X, y)\n",
    "        print('Done')\n",
    "        selector_0 = SelectFromModel(model, \n",
    "                                     threshold=-np.inf,\n",
    "                                     prefit=True,\n",
    "                                     max_features=max_features)\n",
    "        feature_importances_0[i] = selector_0.get_support(indices=True)\n",
    "\n",
    "        selector_1 = SelectFromModel(model, \n",
    "                                     threshold=-np.inf,\n",
    "                                     prefit=True,\n",
    "                                     max_features=max_features//2)\n",
    "        feature_importances_1[i] = selector_1.get_support(indices=True)\n",
    "\n",
    "        selector_2 = SelectFromModel(model, \n",
    "                                     threshold=-np.inf,\n",
    "                                     prefit=True,\n",
    "                                     max_features=max_features//4)\n",
    "        feature_importances_2[i] = selector_2.get_support(indices=True)\n",
    "\n",
    "    tfi_0 = feature_importances_0.flatten()\n",
    "    tfi_1 = feature_importances_1.flatten()\n",
    "    tfi_2 = feature_importances_2.flatten()\n",
    "\n",
    "    u_0 = np.unique(tfi_0)\n",
    "    u_1 = np.unique(tfi_1)\n",
    "    u_2 = np.unique(tfi_2)\n",
    "\n",
    "    fi_dict = {f'feature_importances_{max_features}': u_0.astype(int),\n",
    "               f'feature_importances_{max_features//2}': u_1.astype(int),\n",
    "               f'feature_importances_{max_features//4}': u_2.astype(int)}\n",
    "\n",
    "    return fi_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt0E-hfuJK0T"
   },
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qb_W1JWaj498"
   },
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self,\n",
    "                 path: str = 'ML_Project/dataset',\n",
    "                 years: list = [2014, 2015, 2016, 2017, 2018],\n",
    "                 nan_handler: list = ['threshold'],\n",
    "                 zero_handler: list = ['threshold', 'replace'],\n",
    "                 imputation_handler: str = 'XGB',\n",
    "                 outlier_handler: list = ['quantile', 'IForest'],\n",
    "                 config_dict: dict = {}\n",
    "                 ):\n",
    "        self.path = path\n",
    "        self.years = years\n",
    "        self.nan_handler = nan_handler\n",
    "        self.zero_handler = zero_handler\n",
    "        self.imputation_handler = imputation_handler\n",
    "        self.outlier_handler = outlier_handler\n",
    "        self.config_dict = config_dict\n",
    "\n",
    "        self.df = None\n",
    "        self.year = None\n",
    "        self.init_shape = None\n",
    "        self.class_col = None\n",
    "        self.sector_col = None\n",
    "        self.sector_ids = None\n",
    "        self.sector_mapper = None\n",
    "\n",
    "        self.dfs = []\n",
    "        self.intersec_cols = None\n",
    "\n",
    "    def create_dataset(self):\n",
    "        # loop over all years\n",
    "        for year in self.years:\n",
    "            self.__print_sep(60, '#', '\\n')\n",
    "\n",
    "            # generate single dataframe\n",
    "            self.prepare_single_dataframe(year)\n",
    "\n",
    "            # append dataframe\n",
    "            self.dfs.append(self.df)\n",
    "\n",
    "        # print seperators\n",
    "        self.__print_sep(60, '#')\n",
    "        self.__print_sep(60, '#', '\\n')\n",
    "\n",
    "        # get intersecting columns\n",
    "        self.get_intersecting_columns()\n",
    "\n",
    "        # concatenate dataframes to one\n",
    "        df = self.concat_intersecting_dfs()\n",
    "\n",
    "        # factorize 'Sector' column\n",
    "        df = self.factorize_col(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prepare_single_dataframe(self, year: int):\n",
    "        # load data\n",
    "        self.load_df(year)\n",
    "\n",
    "        # store and drop 'Class' column\n",
    "        self.store_drop_class_col()\n",
    "\n",
    "        # handle NaNs\n",
    "        self.handle_nans()\n",
    "\n",
    "        # handle zeros\n",
    "        self.handle_zeros()\n",
    "\n",
    "        # factorize 'Sector' column and store its mapper\n",
    "        self.store_factorize_sector_col()\n",
    "\n",
    "        # impute NaNs\n",
    "        self.impute_nans()\n",
    "\n",
    "        # handle outliers\n",
    "        self.handle_outliers()\n",
    "\n",
    "        # drop VAR column\n",
    "        self.drop_var_col()\n",
    "\n",
    "        # scale data\n",
    "        self.scale_data()\n",
    "\n",
    "        # convert to float 32 and join\n",
    "        self.convert_join()\n",
    "\n",
    "        # map 'Sector' back to string representation\n",
    "        self.map_sector_inv()\n",
    "\n",
    "    def load_df(self, year: int):\n",
    "        self.__print_header(f'LOADING {year}')\n",
    "\n",
    "        self.year = year\n",
    "        print(f'Loading {year}_Financial_Data.csv into a DataFrame', end=' - ')\n",
    "        self.df = pd.read_csv(os.path.join(self.path, f'{year}_Financial_Data.csv'),\n",
    "                              index_col=0)\n",
    "        print('COMPLETE')\n",
    "        self.init_shape = self.df.shape\n",
    "\n",
    "        # drop rows with no info\n",
    "        print('Dropping rows with NaNs only', end=' - ')\n",
    "        rows = self.df.shape[0]\n",
    "        self.df.dropna(how='all', inplace=True)\n",
    "        print(f'{self.df.shape[0] - rows} rows dropped - COMPLETE')\n",
    "\n",
    "        self.__print_summary(f'Initial DataFrame shape: {self.df.shape}')\n",
    "\n",
    "    def store_drop_class_col(self):\n",
    "        self.class_col = self.df['Class'].astype('int8')\n",
    "        self.df.drop(columns=['Class'], inplace=True)\n",
    "\n",
    "    def store_factorize_sector_col(self):\n",
    "        self.sector_col = self.df['Sector']\n",
    "        self.sector_ids, self.sector_mapper = pd.factorize(self.df['Sector'])\n",
    "        self.df['Sector'] = self.sector_ids\n",
    "\n",
    "    def factorize_col(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        sector_ids, self.sector_mapper = pd.factorize(df['Sector'])\n",
    "        df['Sector'] = sector_ids\n",
    "\n",
    "        return df\n",
    "\n",
    "    def handle_nans(self, round: int = 1):\n",
    "        if round == 1 and self.nan_handler[0]:\n",
    "            self.__print_header(f'HANDLING NANS')\n",
    "\n",
    "        if 'threshold' in self.nan_handler:\n",
    "            nans_thres = self.config_dict[self.year]['nans_thres']\n",
    "            print(f'Dropping columns with more than {int(nans_thres * 100)}% NaNs', end=' - ')\n",
    "            cols = self.df.shape[1]\n",
    "            self.df = self.df.loc[:, self.df.isnull().mean() < nans_thres]  # drop nans\n",
    "            print(f'{cols - self.df.shape[1]} columns dropped - COMPLETE')\n",
    "            self.__print_sep(60, '~', '\\n')\n",
    "\n",
    "    def handle_zeros(self):\n",
    "        self.__print_header(f'HANDLING ZEROS')\n",
    "\n",
    "        if 'threshold' in self.zero_handler:\n",
    "            zeros_thres = self.config_dict[self.year]['zeros_thres']\n",
    "            print(f'Dropping columns with more than {int(zeros_thres * 100)}% zeros', end=' - ')\n",
    "            cols = self.df.shape[1]\n",
    "            self.df = self.df.loc[:, (self.df == 0).mean() < zeros_thres]  # drop zeros\n",
    "            print(f'{cols - self.df.shape[1]} columns dropped - COMPLETE')\n",
    "\n",
    "        if 'replace' in self.zero_handler:\n",
    "            print(f'Replacing zeros with NaNs', end=' - ')\n",
    "            z_count = (self.df == 0).sum().sum()\n",
    "            self.df = self.df.replace({0: np.nan})\n",
    "            z_count_after = (self.df == 0).sum().sum()\n",
    "            print(f'{z_count - z_count_after} zeros replaced - COMPLETE')\n",
    "\n",
    "        if 'replace' in self.zero_handler and 'drop_again' in self.zero_handler:\n",
    "            self.handle_nans(round=2)\n",
    "\n",
    "        da = self.init_shape[1] - self.df.shape[1]\n",
    "        sc = self.df.shape\n",
    "        self.__print_summary(f'Total amount of columns dropped: {da} - Current shape: {sc}')\n",
    "\n",
    "    def impute_nans(self):\n",
    "        self.__print_header(f'IMPUTE NANS')\n",
    "\n",
    "        if self.imputation_handler == 'KNN':\n",
    "            print(f'Imputing {self.df.isnull().sum().sum()} NaNs with KNN', end=' - ')\n",
    "            imputer = KNNImputer(n_neighbors=20, weights='distance', metric='nan_euclidean', copy=True)\n",
    "            np_imp = imputer.fit_transform(self.df)\n",
    "            self.df = pd.DataFrame(np_imp, columns=self.df.columns, index=self.df.index)\n",
    "            print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
    "\n",
    "        elif self.imputation_handler == 'XGB':\n",
    "            print(f'Imputing {self.df.isnull().sum().sum()} NaNs with XGB', end=' - ')\n",
    "            imputer = NaNImputer(verbose=False, multiprocessing_load=1)\n",
    "            self.df = imputer.impute(self.df)\n",
    "            print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
    "            cols = self.df.shape[1]\n",
    "            self.df.dropna(axis=1, inplace=True)  # remove columns that still have NaNs inside\n",
    "            print(f'Removing {cols - self.df.shape[1]} columns such that', end=' ')\n",
    "            print(f'{self.df.isnull().sum().sum()} NaNs are left - COMPLETE')\n",
    "\n",
    "        elif self.imputation_handler == 'mean':\n",
    "            print(f'Imputing NaNs with mean')\n",
    "\n",
    "            # drop rows where the 'Revenue' is unknown (NaNs)\n",
    "            print(f'Dropping rows that have no information about revenue', end=' - ')\n",
    "            rows = self.df.shape[0]\n",
    "            self.df.drop(self.df[self.df['Revenue'].isnull()].index, inplace=True)\n",
    "            print(f'{rows - self.df.shape[0]} rows dropped - COMPLETE')\n",
    "\n",
    "            # introduce new column that describes revenue ranges\n",
    "            range_cond = [(self.df['Revenue'] <= 1e6),\n",
    "                          (self.df['Revenue'] > 1e6) & (self.df['Revenue'] <= 1e7),\n",
    "                          (self.df['Revenue'] > 1e7) & (self.df['Revenue'] <= 1e8),\n",
    "                          (self.df['Revenue'] > 1e8) & (self.df['Revenue'] <= 1e9),\n",
    "                          (self.df['Revenue'] > 1e9)]\n",
    "            self.df['Revenue Range'] = np.select(range_cond, [0, 1, 2, 3, 4])\n",
    "\n",
    "            # store sector column to add it after groupby\n",
    "            sector_col = self.df['Sector']\n",
    "\n",
    "            # group by sector and then revenue range and impute nans with means\n",
    "            nans_count = self.df.isnull().sum().sum()\n",
    "            self.df = self.df.groupby(['Sector', 'Revenue Range']).transform(lambda x: x.fillna(x.mean()))\n",
    "            nans_count_after = self.df.isnull().sum().sum()\n",
    "            print(f'{nans_count - nans_count_after} NaNs imputed', end=' - ')\n",
    "            print(f'{nans_count_after} NaNs left - COMPLETE')\n",
    "\n",
    "            # add sector column again\n",
    "            self.df['Sector'] = sector_col\n",
    "\n",
    "            # if there are still NaNs left, remove the corresponding row(s)\n",
    "            if nans_count_after > 0:\n",
    "                print(f'Dropping rows with remaining NaNs', end=' - ')\n",
    "                row_count = self.df.shape[0]\n",
    "                self.df = self.df.dropna(axis=0)\n",
    "                print(f'{row_count - self.df.shape[0]} row(s) dropped', end=' - ')\n",
    "                print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
    "\n",
    "        self.__print_sep(60, '~', '\\n')\n",
    "\n",
    "    def handle_outliers(self):\n",
    "        self.__print_header('HANDLE OUTLIERS')\n",
    "\n",
    "        if 'quantile' in self.outlier_handler:\n",
    "            rows_before = self.df.shape[0]\n",
    "            cut_lower = self.config_dict[self.year]['cut_lower']\n",
    "            cut_upper = self.config_dict[self.year]['cut_upper']\n",
    "            # drop rows with an unnaturally high price variance\n",
    "            print(f'Dropping rows with a price variance outside the {cut_lower} - {cut_upper} quantile range',\n",
    "                  end=' - ')\n",
    "            col = f'{self.year + 1} PRICE VAR [%]'\n",
    "            outs = self.df[col].between(self.df[col].quantile(cut_lower),\n",
    "                                        self.df[col].quantile(cut_upper))\n",
    "            self.df.drop(self.df[~outs].index, inplace=True)\n",
    "            print(f'{rows_before - self.df.shape[0]} rows dropped - COMPLETE')\n",
    "\n",
    "        if 'IForest' in self.outlier_handler:\n",
    "            print('Using Isolation Forest to detect outliers', end=' - ')\n",
    "            contamination = self.config_dict[self.year]['IForest_contamination']\n",
    "            clf = IForest(contamination=contamination,\n",
    "                          random_state=42,\n",
    "                          n_jobs=-1)\n",
    "            clf.fit(self.df.values)\n",
    "            y_pred = clf.predict(self.df.values)\n",
    "            idx_y_pred = [i for i in range(self.df.shape[0]) if y_pred[i] == 0]\n",
    "            self.df = self.df.iloc[idx_y_pred, :]\n",
    "            print(f'{sum(y_pred)} outliers removed - COMPLETE')\n",
    "\n",
    "        self.__print_sep(60, '~', '\\n')\n",
    "\n",
    "    def scale_data(self):\n",
    "        self.__print_header('SCALING DATA')\n",
    "\n",
    "        print(f'Scaling data', end=' - ')\n",
    "        sector_col = self.df['Sector'] if 'Sector' in self.df.columns else None\n",
    "        scaler = StandardScaler()\n",
    "        np_scaled = scaler.fit_transform(self.df)\n",
    "        self.df = pd.DataFrame(np_scaled,\n",
    "                               columns=self.df.columns,\n",
    "                               index=self.df.index)\n",
    "\n",
    "        if sector_col is not None:\n",
    "            self.df['Sector'] = sector_col\n",
    "        print('COMPLETE')\n",
    "\n",
    "    def drop_var_col(self):\n",
    "        self.df.drop(columns=[f'{self.year + 1} PRICE VAR [%]'], inplace=True)\n",
    "\n",
    "    def convert_join(self):\n",
    "        # convert to float32\n",
    "        self.df = self.df.astype('float32')\n",
    "\n",
    "        # join 'Class' column\n",
    "        self.df = self.df.join(self.class_col)\n",
    "\n",
    "        self.__print_summary(f'Final DataFrame shape: {self.df.shape}')\n",
    "\n",
    "    def map_sector_inv(self):\n",
    "        mapper = {i: sector for i, sector in enumerate(self.sector_mapper)}\n",
    "        self.df['Sector'] = self.df['Sector'].apply(lambda x: mapper[x])\n",
    "\n",
    "    def get_intersecting_columns(self):\n",
    "        df1 = self.dfs[0].columns\n",
    "        df2 = self.dfs[1].columns\n",
    "        df3 = self.dfs[2].columns\n",
    "        df4 = self.dfs[3].columns\n",
    "        df5 = self.dfs[4].columns\n",
    "\n",
    "        self.intersec_cols = df1 & df2 & df3 & df4 & df5\n",
    "        print(f'Found {len(self.intersec_cols)} intersecting columns!')\n",
    "\n",
    "    def concat_intersecting_dfs(self):\n",
    "        df1 = self.dfs[0][self.intersec_cols]\n",
    "        df2 = self.dfs[1][self.intersec_cols]\n",
    "        df3 = self.dfs[2][self.intersec_cols]\n",
    "        df4 = self.dfs[3][self.intersec_cols]\n",
    "        df5 = self.dfs[4][self.intersec_cols]\n",
    "\n",
    "        df = pd.concat([df1, df2, df3, df4, df5])\n",
    "        print(f'Concatenated DataFrame into shape: {df.shape}')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def __print_summary(self, info: str):\n",
    "        self.__print_sep()\n",
    "        print(info)\n",
    "        self.__print_sep(60, '~', '\\n')\n",
    "\n",
    "    def __print_sep(self, n: int = 60, c: str = '-', nl: str = ''):\n",
    "        print(n * c + nl)\n",
    "\n",
    "    def __print_header(self, header: str):\n",
    "        rem = 60 - 25 - 2 - len(header)\n",
    "        print(f'{25 * \"~\"} {header} {rem * \"~\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNHa5JNemgs-"
   },
   "source": [
    "# Compare approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhDP7N_Bs9Ry"
   },
   "source": [
    "## Set config dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OwwwrQwgiO-"
   },
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    2014: {\n",
    "        'nans_thres': 0.08, \n",
    "        'zeros_thres': 0.08,\n",
    "        'cut_lower': 0.01,\n",
    "        'cut_upper': 0.99,\n",
    "        'IForest_contamination': 0.02\n",
    "    },\n",
    "    2015: {\n",
    "        'nans_thres': 0.10, \n",
    "        'zeros_thres': 0.08,\n",
    "        'cut_lower': 0.01,\n",
    "        'cut_upper': 0.99,\n",
    "        'IForest_contamination': 0.02\n",
    "    },\n",
    "    2016: {\n",
    "        'nans_thres': 0.16, \n",
    "        'zeros_thres': 0.08,\n",
    "        'cut_lower': 0.01,\n",
    "        'cut_upper': 0.99,\n",
    "        'IForest_contamination': 0.02\n",
    "    },\n",
    "    2017: {\n",
    "        'nans_thres': 0.16, \n",
    "        'zeros_thres': 0.08,\n",
    "        'cut_lower': 0.01,\n",
    "        'cut_upper': 0.99,\n",
    "        'IForest_contamination': 0.02\n",
    "    },\n",
    "    2018: {\n",
    "        'nans_thres': 0.08, \n",
    "        'zeros_thres': 0.08,\n",
    "        'cut_lower': 0.01,\n",
    "        'cut_upper': 0.99,\n",
    "        'IForest_contamination': 0.02\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otz1ng9emkbF"
   },
   "source": [
    "## Approach 1\n",
    "\n",
    "1.   *NaN threshold*\n",
    "2.   *zero threshold*\n",
    "3.   *mean imputation*\n",
    "4.   *outlier quantile cut*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "XyMm5zQ8gZnV",
    "outputId": "ebee1930-131c-4113-b1de-b71e443f4411"
   },
   "outputs": [],
   "source": [
    "dataset_1 = DataSet(config_dict=config_dict,\n",
    "                    nan_handler = ['threshold'],\n",
    "                    zero_handler = ['threshold'],\n",
    "                    imputation_handler = 'mean',\n",
    "                    outlier_handler = ['quantile'])\n",
    "\n",
    "# generate DataFrame\n",
    "df_1 = dataset_1.create_dataset()\n",
    "\n",
    "# use full df for cross validation vanilla test\n",
    "X_1, y_1 = df_1.drop(columns=['Class']), df_1['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cw4By1i5oz-e"
   },
   "outputs": [],
   "source": [
    "test_vanilla(X_1, y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4SGEHkIo7kQ"
   },
   "source": [
    "## Approach 2\n",
    "\n",
    "\n",
    "1.   *zero threshold*\n",
    "2.   *replace zeros with NaNs*\n",
    "3.   *KNN imputation*\n",
    "4.   *outlier quantile cut*\n",
    "5.   *Isolation Forest outlier handling*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNjNTA_PqSil"
   },
   "outputs": [],
   "source": [
    "dataset_2 = DataSet(config_dict=config_dict,\n",
    "                    nan_handler = [],\n",
    "                    zero_handler = ['threshold', 'replace'],\n",
    "                    imputation_handler = 'KNN',\n",
    "                    outlier_handler = ['quantile', 'IForest'])\n",
    "\n",
    "# generate DataFrame\n",
    "df_2 = dataset_2.create_dataset()\n",
    "\n",
    "# use full df for cross validation vanilla test\n",
    "X_2, y_2 = df_2.drop(columns=['Class']), df_2['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKZb3loZrFCr"
   },
   "outputs": [],
   "source": [
    "test_vanilla(X_2, y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKkOx65LrH0b"
   },
   "source": [
    "## Approach 3\n",
    "\n",
    "1.   *replace zeros with NaNs*\n",
    "3.   *XGB imputation*\n",
    "4.   *outlier quantile cut*\n",
    "5.   *Isolation Forest outlier handling*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-dLVG_8rceJ"
   },
   "outputs": [],
   "source": [
    "dataset_3 = DataSet(config_dict=config_dict,\n",
    "                    nan_handler = [],\n",
    "                    zero_handler = ['replace'],\n",
    "                    imputation_handler = 'XGB',\n",
    "                    outlier_handler = ['quantile', 'IForest'])\n",
    "\n",
    "# generate DataFrame\n",
    "df_3 = dataset_3.create_dataset()\n",
    "\n",
    "# use full df for cross validation vanilla test\n",
    "X_3, y_3= df_3.drop(columns=['Class']), df_3['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOvsf0o5ro21"
   },
   "outputs": [],
   "source": [
    "test_vanilla(X_3, y_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Khb0Dm11rpyq"
   },
   "source": [
    "## Approach 4\n",
    "\n",
    "1.   *NaN threshold*\n",
    "2.   *zero threshold*\n",
    "3.   *replace zeros with NaNs*\n",
    "4.   *NaN threshold again*\n",
    "5.   *KNN imputation*\n",
    "6.   *outlier quantile cut*\n",
    "7.   *Isolation Forest outlier handling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9STn0QbfsV6D"
   },
   "outputs": [],
   "source": [
    "dataset_4 = DataSet(config_dict=config_dict,\n",
    "                    nan_handler = [],\n",
    "                    zero_handler = ['replace'],\n",
    "                    imputation_handler = 'XGB',\n",
    "                    outlier_handler = ['quantile', 'IForest'])\n",
    "\n",
    "# generate DataFrame\n",
    "df_4 = dataset_4.create_dataset()\n",
    "\n",
    "# use full df for cross validation vanilla test\n",
    "X_4, y_4= df_4.drop(columns=['Class']), df_4['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ay4t_2VOsf8p"
   },
   "outputs": [],
   "source": [
    "test_vanilla(X_4, y_4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ml_project_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
