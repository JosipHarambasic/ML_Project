{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_project_notebook (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosipHarambasic/ML_Project/blob/master/ml_project_notebook_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H2YSwnoPX6y",
        "outputId": "620320c8-4ae9-443a-c0af-c72dd84d2bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ML_Project'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (112/112), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 112 (delta 44), reused 91 (delta 23), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (112/112), 44.09 MiB | 12.15 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/JosipHarambasic/ML_Project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "xhX8N-rzP9Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install verstack\n",
        "!pip install catboost\n",
        "!pip install pyod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NNoo1k3ZmBBO",
        "outputId": "d9c34835-89af-4dce-e541-935ae1b963ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting verstack\n",
            "  Downloading verstack-3.0.3.tar.gz (9.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6 MB 11.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from verstack) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from verstack) (1.21.5)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (from verstack) (0.90)\n",
            "Collecting scikit-learn==1.0.1\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.6 MB/s \n",
            "\u001b[?25hCollecting lightgbm==3.3.0\n",
            "  Downloading lightgbm-3.3.0-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 43.6 MB/s \n",
            "\u001b[?25hCollecting optuna==2.10.0\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[K     |████████████████████████████████| 308 kB 53.9 MB/s \n",
            "\u001b[?25hCollecting plotly==5.3.1\n",
            "  Downloading plotly-5.3.1-py2.py3-none-any.whl (23.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9 MB 77.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from verstack) (3.2.2)\n",
            "Collecting python-dateutil==2.8.1\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 49.6 MB/s \n",
            "\u001b[?25hCollecting holidays==0.11.3.1\n",
            "  Downloading holidays-0.11.3.1-py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 15.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (from verstack) (0.14.0)\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 489.6 MB 22 kB/s \n",
            "\u001b[?25hCollecting keras==2.7.0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: convertdate>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from holidays==0.11.3.1->verstack) (2.4.0)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays==0.11.3.1->verstack) (0.2.1)\n",
            "Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays==0.11.3.1->verstack) (2.2.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.3.0->verstack) (0.37.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.3.0->verstack) (1.4.1)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.10.0->verstack) (21.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna==2.10.0->verstack) (4.63.0)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 68.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna==2.10.0->verstack) (3.13)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.10.0->verstack) (1.4.32)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly==5.3.1->verstack) (8.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly==5.3.1->verstack) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.1->verstack) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.1->verstack) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.44.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (0.24.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (3.10.0.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (13.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.1.2)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 77.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.6.3)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.0.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->verstack) (1.1.0)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.3.0->holidays==0.11.3.1->verstack) (0.5.11)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0->verstack) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna==2.10.0->verstack) (3.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.10.0->verstack) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.10.0->verstack) (4.11.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->verstack) (3.3.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->verstack) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->verstack) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->verstack) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->verstack) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==2.10.0->verstack) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->verstack) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->verstack) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->verstack) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->verstack) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->verstack) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->verstack) (3.2.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==2.10.0->verstack) (5.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.10.0->verstack) (3.2.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 70.0 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.0-py3-none-any.whl (150 kB)\n",
            "\u001b[K     |████████████████████████████████| 150 kB 54.9 MB/s \n",
            "\u001b[?25hCollecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.10.0->verstack) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.10.0->verstack) (21.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==2.10.0->verstack) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->verstack) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->verstack) (1.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->verstack) (2018.9)\n",
            "Building wheels for collected packages: verstack, pyperclip\n",
            "  Building wheel for verstack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for verstack: filename=verstack-3.0.3-py3-none-any.whl size=73100 sha256=0b1ff176067dccdf7ee5642999e2c2c00e7f9914200c53c44996a41f83cd4806\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/7d/41/2b9ac43b55213e71352931fc34878f7dd9d10b887555a625f7\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=b955f65d72f2743786799e17c4b49ceb5c7de06c2d201b3dbf0a96e3a5df3bbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built verstack pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, python-dateutil, Mako, cmd2, autopage, tensorflow-estimator, scikit-learn, keras, gast, colorlog, cmaes, cliff, alembic, tensorflow, plotly, optuna, lightgbm, holidays, verstack\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.5.0\n",
            "    Uninstalling plotly-5.5.0:\n",
            "      Successfully uninstalled plotly-5.5.0\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Attempting uninstall: holidays\n",
            "    Found existing installation: holidays 0.10.5.2\n",
            "    Uninstalling holidays-0.10.5.2:\n",
            "      Successfully uninstalled holidays-0.10.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Mako-1.2.0 alembic-1.7.7 autopage-0.5.0 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.0 colorlog-6.6.0 gast-0.4.0 holidays-0.11.3.1 keras-2.7.0 lightgbm-3.3.0 optuna-2.10.0 pbr-5.8.1 plotly-5.3.1 pyperclip-1.8.2 python-dateutil-2.8.1 scikit-learn-1.0.1 stevedore-3.5.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 verstack-3.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.0.5-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.6 MB 49 kB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (3.10.0.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.5\n",
            "Collecting pyod\n",
            "  Downloading pyod-0.9.9.tar.gz (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyod) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pyod) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from pyod) (1.21.5)\n",
            "Requirement already satisfied: numba>=0.35 in /usr/local/lib/python3.7/dist-packages (from pyod) (0.51.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from pyod) (1.4.1)\n",
            "Requirement already satisfied: scikit_learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from pyod) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pyod) (1.15.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod) (0.10.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.35->pyod) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.35->pyod) (0.34.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>=0.20.0->pyod) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->pyod) (3.10.0.2)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pyod) (0.5.2)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pyod) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels->pyod) (2018.9)\n",
            "Building wheels for collected packages: pyod\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-0.9.9-py3-none-any.whl size=139325 sha256=b31d5e02da245f361cf6a94b123a54f7a6ae4572f0f1ec7a45c6a122f59be4c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/32/f0/0dc3050775e77b6661a116b70817b02b4305fa253269d6d998\n",
            "Successfully built pyod\n",
            "Installing collected packages: pyod\n",
            "Successfully installed pyod-0.9.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "from scipy import stats\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from verstack import NaNImputer\n",
        "from pyod.models.iforest import IForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "MPJ1fGs7Qhgy",
        "pycharm": {
          "is_executing": true
        }
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # do not show warnings"
      ],
      "metadata": {
        "id": "oo0QnAcQjjZG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers"
      ],
      "metadata": {
        "id": "xpuGNAJSP_Br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model related"
      ],
      "metadata": {
        "id": "O1oIE84yslGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_vanilla(X, y):\n",
        "    ETC = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
        "    RFC = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    GBC = GradientBoostingClassifier(random_state=42)\n",
        "    LGBMC = LGBMClassifier(random_state=42, n_jobs=-1)\n",
        "    XGBC = XGBClassifier(seed=42)\n",
        "    CBC = CatBoostClassifier(random_state=42, verbose=0)\n",
        "    KNN = KNeighborsClassifier(n_jobs=-1)\n",
        "    SGDC = SGDClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "    names = ['ETC', 'RFC', 'GBC', 'LGBMC', 'XGBC', 'CBC', 'KNN', 'SGDC']\n",
        "    models = [ETC, RFC, GBC, LGBMC, XGBC, CBC, KNN, SGDC]\n",
        "\n",
        "    for name, model in zip(names, models):\n",
        "        metric = 'Accuracy'\n",
        "        cv = RepeatedKFold(n_splits=5, \n",
        "                          n_repeats=1, \n",
        "                          random_state=42)\n",
        "        grid = GridSearchCV(estimator=model, \n",
        "                            param_grid={}, \n",
        "                            scoring=metric.lower(),\n",
        "                            cv=cv, \n",
        "                            verbose=0,\n",
        "                            n_jobs=-1)\n",
        "        grid.fit(X, y)\n",
        "        print(f\"{metric} for {name}: {grid.best_score_}\")"
      ],
      "metadata": {
        "id": "HSc-4dDxsnuV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_importances(X, \n",
        "                            y, \n",
        "                            max_features: int) -> dict:\n",
        "    ETC = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
        "    RFC = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    GBC = GradientBoostingClassifier(random_state=42)\n",
        "    LGBMC = LGBMClassifier(random_state=42, n_jobs=-1)\n",
        "    XGBC = XGBClassifier(seed=42)\n",
        "    CBC = CatBoostClassifier(random_state=42, verbose=0)\n",
        "    SGDC = SGDClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "    names = ['ETC', 'RFC', 'GBC', 'LGBMC', 'XGBC', 'CBC', 'SGDC']\n",
        "    models = [ETC, RFC, GBC, LGBMC, XGBC, CBC, SGDC]\n",
        "\n",
        "    feature_importances_0 = np.zeros((7, max_features))\n",
        "    feature_importances_1 = np.zeros((7, max_features//2))\n",
        "    feature_importances_2 = np.zeros((7, max_features//4))\n",
        "    \n",
        "    for i, name, model in zip(range(len(models)), names, models):\n",
        "        print(f'Fitting {name}...', end=' ')\n",
        "        model.fit(X, y)\n",
        "        print('Done')\n",
        "        selector_0 = SelectFromModel(model, \n",
        "                                     threshold=-np.inf,\n",
        "                                     prefit=True,\n",
        "                                     max_features=max_features)\n",
        "        feature_importances_0[i] = selector_0.get_support(indices=True)\n",
        "\n",
        "        selector_1 = SelectFromModel(model, \n",
        "                                     threshold=-np.inf,\n",
        "                                     prefit=True,\n",
        "                                     max_features=max_features//2)\n",
        "        feature_importances_1[i] = selector_1.get_support(indices=True)\n",
        "\n",
        "        selector_2 = SelectFromModel(model, \n",
        "                                     threshold=-np.inf,\n",
        "                                     prefit=True,\n",
        "                                     max_features=max_features//4)\n",
        "        feature_importances_2[i] = selector_2.get_support(indices=True)\n",
        "\n",
        "    tfi_0 = feature_importances_0.flatten()\n",
        "    tfi_1 = feature_importances_1.flatten()\n",
        "    tfi_2 = feature_importances_2.flatten()\n",
        "\n",
        "    u_0 = np.unique(tfi_0)\n",
        "    u_1 = np.unique(tfi_1)\n",
        "    u_2 = np.unique(tfi_2)\n",
        "\n",
        "    fi_dict = {f'feature_importances_{max_features}': u_0.astype(int),\n",
        "               f'feature_importances_{max_features//2}': u_1.astype(int),\n",
        "               f'feature_importances_{max_features//4}': u_2.astype(int)}\n",
        "\n",
        "    return fi_dict"
      ],
      "metadata": {
        "id": "kt2K5JTOmaT9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset class"
      ],
      "metadata": {
        "id": "Mt0E-hfuJK0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataSet:\n",
        "    \"\"\"\n",
        "    Class handling all preprocessing with different possible configurations\n",
        "\n",
        "    :param path: Path to datasets\n",
        "    :param years: List of years to be considered\n",
        "    :param nan_handler: List of NaN handling methods - Options: 'threshold', empty\n",
        "    :param zero_handler: List of zero handling methods - Options: 'threshold', 'replace', 'drop_again', empty\n",
        "                         - Is stackable, but 'drop_again' needs the 'replace' argument - e.g. ['replace', 'drop_again']\n",
        "    :param imputation_handler: Imputation method - Options: 'mean', 'KNN', 'XGB'\n",
        "    :param outlier_handler: List of outlier handling methods - Options: 'quantile', 'IForest'\n",
        "                            - Is stackable - e.g. ['quantile', 'IForest']\n",
        "    :param config_dict: Dictionary that defines the thresholds and contamination (for IForest) for each year.\n",
        "                        It has to be structured the following way:\n",
        "\n",
        "                                      config_dict = {\n",
        "                                                      2014: {\n",
        "                                                          'nans_thres': float (e.g. 0.08), \n",
        "                                                          'zeros_thres': float (e.g. 0.08),\n",
        "                                                          'cut_lower': float (e.g. 0.01),\n",
        "                                                          'cut_upper': float (e.g. 0.99),\n",
        "                                                          'IForest_contamination': float (e.g. 0.02)\n",
        "                                                      },\n",
        "                                                    ...\n",
        "                                                    }\n",
        "                        :key nans_thres: Sets the NaN threshold, such that columns that surpass this threshold\n",
        "                                         are removed (e.g. 0.08 - allowing up to 8% NaNs in columns)\n",
        "                        :key zeros_thres: Sets zero threshold, such that columns that surpass this threshold\n",
        "                                          are removed (e.g. 0.08 - allowing up to 8% zeros in columns)\n",
        "                        :key cut_lower: Sets lower bound, such that rows that have a price var which falls below \n",
        "                                        the quantile are removed (e.g. 0.01)\n",
        "                        :key cut_upper: Sets upper bound, such that rows that have a price var which is above \n",
        "                                        the quantile are removed (e.g. 0.01)\n",
        "                        : key IForest_contamination: Sets the contamination percentage of the dataset (e.g 0.02 - 2% outliers)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 path: str = '',\n",
        "                 benchmark = 2.5,\n",
        "                 sp_index: list = [-0.73, 9.54, 19.42, -6.24, 28.88],\n",
        "                 years: list = [2014, 2015, 2016, 2017, 2018],\n",
        "                 nan_handler: list = ['threshold'],\n",
        "                 zero_handler: list = ['threshold', 'replace'],\n",
        "                 imputation_handler: str = 'XGB',\n",
        "                 outlier_handler: list = ['quantile', 'IForest'],\n",
        "                 config_dict: dict = {}\n",
        "                 ):\n",
        "        self.path = path\n",
        "        self.years = years\n",
        "        self.nan_handler = nan_handler\n",
        "        self.zero_handler = zero_handler\n",
        "        self.imputation_handler = imputation_handler\n",
        "        self.outlier_handler = outlier_handler\n",
        "        self.config_dict = config_dict\n",
        "        self.sp_index = sp_index\n",
        "        self.benchmark = benchmark\n",
        "\n",
        "        self.df = None\n",
        "        self.year = None\n",
        "        self.init_shape = None\n",
        "        self.class_col = None\n",
        "        self.sector_col = None\n",
        "        self.sector_ids = None\n",
        "        self.sector_mapper = None\n",
        "        self.dfs = []\n",
        "        self.intersec_cols = None\n",
        "\n",
        "    def create_dataset(self):\n",
        "        \"\"\"\n",
        "        Creates the whole dataset:\n",
        "            - Stores dataframe per year in self.dfs\n",
        "            - Finds intersecting columns between all dataframes\n",
        "            - Concatenates all dataframes with the selected columns\n",
        "            - Factorize the 'Sector' column -> Assign ID to each string\n",
        "            - Rename feature names to avoid UTF-8 encoding issues with LGBM\n",
        "        \"\"\"\n",
        "\n",
        "        # loop over all years\n",
        "        counter = 0\n",
        "        buy = self.sp_index[counter] + self.benchmark\n",
        "        sell = self.sp_index[counter] - self.benchmark\n",
        "        for year in self.years:\n",
        "            self.__print_sep(60, '#', '\\n')\n",
        "\n",
        "            # generate single dataframe\n",
        "            self.prepare_single_dataframe(year)\n",
        "\n",
        "            # append dataframe\n",
        "            self.dfs.append(self.df)\n",
        "            counter += 1\n",
        "\n",
        "        # print seperators\n",
        "        self.__print_sep(60, '#')\n",
        "        self.__print_sep(60, '#', '\\n')\n",
        "\n",
        "        # get intersecting columns\n",
        "        self.get_intersecting_columns()\n",
        "\n",
        "        # concatenate dataframes to one\n",
        "        df = self.concat_intersecting_dfs()\n",
        "\n",
        "        # factorize 'Sector' column\n",
        "        df = self.factorize_col(df)\n",
        "\n",
        "        # avoid encoding issues with feature names\n",
        "        df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_single_dataframe(self, year: int):\n",
        "        \"\"\"\n",
        "        Prepares a dataframe for a specific year\n",
        "\n",
        "        :param year: Sets year\n",
        "\n",
        "            - Loads data into dataframe\n",
        "            - Drop the 'Class' column for preprocessing\n",
        "            - Preprocess NaNs\n",
        "            - Preprocess zeros\n",
        "            - Factorize 'Sector' column for imputation\n",
        "            - Impute NaNs\n",
        "            - Detect and remove outliers\n",
        "            - Scale data\n",
        "            - Convert data to float32 and join back 'Class' column\n",
        "            - Map back the 'Sector' column\n",
        "        \"\"\"\n",
        "        # load data\n",
        "        self.load_df(year)\n",
        "\n",
        "        # store and drop 'Class' column\n",
        "        self.store_drop_class_col()\n",
        "\n",
        "        # handle NaNs\n",
        "        self.handle_nans()\n",
        "\n",
        "        # handle zeros\n",
        "        self.handle_zeros()\n",
        "\n",
        "        # factorize 'Sector' column and store its mapper\n",
        "        self.store_factorize_sector_col()\n",
        "\n",
        "        # impute NaNs\n",
        "        self.impute_nans()\n",
        "\n",
        "        # handle outliers\n",
        "        self.handle_outliers()\n",
        "\n",
        "        # drop VAR column\n",
        "        self.drop_var_col()\n",
        "\n",
        "        # scale data\n",
        "        self.scale_data()\n",
        "\n",
        "        # convert to float 32 and join\n",
        "        self.convert_join()\n",
        "\n",
        "        # map 'Sector' back to string representation\n",
        "        self.map_sector_inv()\n",
        "\n",
        "    def load_df(self, year: int):\n",
        "        self.__print_header(f'LOADING {year}')\n",
        "        self.year = year\n",
        "        print(f'Loading {year}_Financial_Data.csv into a DataFrame', end=' - ')\n",
        "        self.df = pd.read_csv(f'/{year}_Financial_Data.csv',\n",
        "                              index_col=0)\n",
        "        print('COMPLETE')\n",
        "        self.init_shape = self.df.shape\n",
        "\n",
        "        # drop rows with no info\n",
        "        print('Dropping rows with NaNs only', end=' - ')\n",
        "        rows = self.df.shape[0]\n",
        "        self.df.dropna(how='all', inplace=True)\n",
        "        print(f'{self.df.shape[0] - rows} rows dropped - COMPLETE')\n",
        "\n",
        "        self.__print_summary(f'Initial DataFrame shape: {self.df.shape}')\n",
        "\n",
        "    def store_drop_class_col(self):\n",
        "        self.class_col = self.df['Class'].astype('int8')\n",
        "        self.df.drop(columns=['Class'], inplace=True)\n",
        "\n",
        "    def store_factorize_sector_col(self):\n",
        "        self.sector_col = self.df['Sector']\n",
        "        self.sector_ids, self.sector_mapper = pd.factorize(self.df['Sector'])\n",
        "        self.df['Sector'] = self.sector_ids\n",
        "\n",
        "    def factorize_col(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        sector_ids, self.sector_mapper = pd.factorize(df['Sector'])\n",
        "        df['Sector'] = sector_ids\n",
        "        df['Sector'] = df['Sector'].astype('float32')\n",
        "        df.loc[df[f'/{self.year} PRICE VAR [%]']>sp500_2015_buy, \"Class\"] = 2\n",
        "        df.loc[df[f'/{self.year} PRICE VAR [%]']<sp500_2015_sell, \"Class\"] = 0\n",
        "        df.loc[(df[f'/{self.year} PRICE VAR [%]'] >= sp500_2015_sell) & \n",
        "           (df[f'/{self.year} PRICE VAR [%]']<=sp500_2015_buy), \"Class\"] = 1\n",
        "\n",
        "        return df\n",
        "\n",
        "    def handle_nans(self, round: int = 1):\n",
        "        if round == 1 and self.nan_handler:\n",
        "            self.__print_header(f'HANDLING NANS')\n",
        "\n",
        "        if 'threshold' in self.nan_handler:\n",
        "            nans_thres = self.config_dict[self.year]['nans_thres']\n",
        "            print(f'Dropping columns with more than {int(nans_thres * 100)}% NaNs', end=' - ')\n",
        "            cols = self.df.shape[1]\n",
        "            self.df = self.df.loc[:, self.df.isnull().mean() < nans_thres]  # drop nans\n",
        "            print(f'{cols - self.df.shape[1]} columns dropped - COMPLETE')\n",
        "            self.__print_sep(60, '~', '\\n')\n",
        "\n",
        "    def handle_zeros(self):\n",
        "        self.__print_header(f'HANDLING ZEROS')\n",
        "\n",
        "        if 'threshold' in self.zero_handler:\n",
        "            zeros_thres = self.config_dict[self.year]['zeros_thres']\n",
        "            print(f'Dropping columns with more than {int(zeros_thres * 100)}% zeros', end=' - ')\n",
        "            cols = self.df.shape[1]\n",
        "            self.df = self.df.loc[:, (self.df == 0).mean() < zeros_thres]  # drop zeros\n",
        "            print(f'{cols - self.df.shape[1]} columns dropped - COMPLETE')\n",
        "\n",
        "        if 'replace' in self.zero_handler:\n",
        "            print(f'Replacing zeros with NaNs', end=' - ')\n",
        "            z_count = (self.df == 0).sum().sum()\n",
        "            self.df = self.df.replace({0: np.nan})\n",
        "            z_count_after = (self.df == 0).sum().sum()\n",
        "            print(f'{z_count - z_count_after} zeros replaced - COMPLETE')\n",
        "\n",
        "        if 'replace' in self.zero_handler and 'drop_again' in self.zero_handler:\n",
        "            self.handle_nans(round=2)\n",
        "\n",
        "        da = self.init_shape[1] - self.df.shape[1]\n",
        "        sc = self.df.shape\n",
        "        self.__print_summary(f'Total amount of columns dropped: {da} - Current shape: {sc}')\n",
        "\n",
        "    def impute_nans(self):\n",
        "        self.__print_header(f'IMPUTE NANS')\n",
        "\n",
        "        if self.imputation_handler == 'KNN':\n",
        "            print(f'Imputing {self.df.isnull().sum().sum()} NaNs with KNN', end=' - ')\n",
        "            imputer = KNNImputer(n_neighbors=20, weights='distance', metric='nan_euclidean', copy=True)\n",
        "            np_imp = imputer.fit_transform(self.df)\n",
        "            self.df = pd.DataFrame(np_imp, columns=self.df.columns, index=self.df.index)\n",
        "            print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
        "\n",
        "        elif self.imputation_handler == 'XGB':\n",
        "            print(f'Imputing {self.df.isnull().sum().sum()} NaNs with XGB', end=' - ')\n",
        "            imputer = NaNImputer(verbose=False, multiprocessing_load=1)\n",
        "            self.df = imputer.impute(self.df)\n",
        "            print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
        "            cols = self.df.shape[1]\n",
        "            self.df.dropna(axis=1, inplace=True)  # remove columns that still have NaNs inside\n",
        "            print(f'Removing {cols - self.df.shape[1]} columns such that', end=' ')\n",
        "            print(f'{self.df.isnull().sum().sum()} NaNs are left - COMPLETE')\n",
        "\n",
        "        elif self.imputation_handler == 'mean':\n",
        "            print(f'Imputing NaNs with mean')\n",
        "\n",
        "            # drop rows where the 'Revenue' is unknown (NaNs)\n",
        "            print(f'Dropping rows that have no information about revenue', end=' - ')\n",
        "            rows = self.df.shape[0]\n",
        "            self.df.drop(self.df[self.df['Revenue'].isnull()].index, inplace=True)\n",
        "            print(f'{rows - self.df.shape[0]} rows dropped - COMPLETE')\n",
        "\n",
        "            # introduce new column that describes revenue ranges\n",
        "            range_cond = [(self.df['Revenue'] <= 1e6),\n",
        "                          (self.df['Revenue'] > 1e6) & (self.df['Revenue'] <= 1e7),\n",
        "                          (self.df['Revenue'] > 1e7) & (self.df['Revenue'] <= 1e8),\n",
        "                          (self.df['Revenue'] > 1e8) & (self.df['Revenue'] <= 1e9),\n",
        "                          (self.df['Revenue'] > 1e9)]\n",
        "            self.df['Revenue Range'] = np.select(range_cond, [0, 1, 2, 3, 4])\n",
        "\n",
        "            # store sector column to add it after groupby\n",
        "            sector_col = self.df['Sector']\n",
        "\n",
        "            # group by sector and then revenue range and impute nans with means\n",
        "            nans_count = self.df.isnull().sum().sum()\n",
        "            self.df = self.df.groupby(['Sector', 'Revenue Range']).transform(lambda x: x.fillna(x.mean()))\n",
        "            nans_count_after = self.df.isnull().sum().sum()\n",
        "            print(f'{nans_count - nans_count_after} NaNs imputed', end=' - ')\n",
        "            print(f'{nans_count_after} NaNs left - COMPLETE')\n",
        "\n",
        "            # add sector column again\n",
        "            self.df['Sector'] = sector_col\n",
        "\n",
        "            # if there are still NaNs left, remove the corresponding row(s)\n",
        "            if nans_count_after > 0:\n",
        "                print(f'Dropping rows with remaining NaNs', end=' - ')\n",
        "                row_count = self.df.shape[0]\n",
        "                self.df = self.df.dropna(axis=0)\n",
        "                print(f'{row_count - self.df.shape[0]} row(s) dropped', end=' - ')\n",
        "                print(f'{self.df.isnull().sum().sum()} NaNs left - COMPLETE')\n",
        "\n",
        "        self.__print_sep(60, '~', '\\n')\n",
        "\n",
        "    def handle_outliers(self):\n",
        "        self.__print_header('HANDLE OUTLIERS')\n",
        "\n",
        "        if 'quantile' in self.outlier_handler:\n",
        "            rows_before = self.df.shape[0]\n",
        "            cut_lower = self.config_dict[self.year]['cut_lower']\n",
        "            cut_upper = self.config_dict[self.year]['cut_upper']\n",
        "            # drop rows with an unnaturally high price variance\n",
        "            print(f'Dropping rows with a price variance outside the {cut_lower} - {cut_upper} quantile range',\n",
        "                  end=' - ')\n",
        "            col = f'{self.year + 1} PRICE VAR [%]'\n",
        "            outs = self.df[col].between(self.df[col].quantile(cut_lower),\n",
        "                                        self.df[col].quantile(cut_upper))\n",
        "            self.df.drop(self.df[~outs].index, inplace=True)\n",
        "            print(f'{rows_before - self.df.shape[0]} rows dropped - COMPLETE')\n",
        "\n",
        "        if 'IForest' in self.outlier_handler:\n",
        "            print('Using Isolation Forest to detect outliers', end=' - ')\n",
        "            contamination = self.config_dict[self.year]['IForest_contamination']\n",
        "            clf = IForest(contamination=contamination,\n",
        "                          random_state=42,\n",
        "                          n_jobs=-1)\n",
        "            clf.fit(self.df.values)\n",
        "            y_pred = clf.predict(self.df.values)\n",
        "            idx_y_pred = [i for i in range(self.df.shape[0]) if y_pred[i] == 0]\n",
        "            self.df = self.df.iloc[idx_y_pred, :]\n",
        "            print(f'{sum(y_pred)} outliers removed - COMPLETE')\n",
        "\n",
        "        self.__print_sep(60, '~', '\\n')\n",
        "\n",
        "    def scale_data(self):\n",
        "        self.__print_header('SCALING DATA')\n",
        "\n",
        "        print(f'Scaling data', end=' - ')\n",
        "        sector_col = self.df['Sector'] if 'Sector' in self.df.columns else None\n",
        "        scaler = StandardScaler()\n",
        "        np_scaled = scaler.fit_transform(self.df)\n",
        "        self.df = pd.DataFrame(np_scaled,\n",
        "                               columns=self.df.columns,\n",
        "                               index=self.df.index)\n",
        "\n",
        "        if sector_col is not None:\n",
        "            self.df['Sector'] = sector_col\n",
        "        print('COMPLETE')\n",
        "\n",
        "    def drop_var_col(self):\n",
        "        self.df.drop(columns=[f'{self.year + 1} PRICE VAR [%]'], inplace=True)\n",
        "\n",
        "    def convert_join(self):\n",
        "        # convert to float32\n",
        "        self.df = self.df.astype('float32')\n",
        "\n",
        "        # join 'Class' column\n",
        "        self.df = self.df.join(self.class_col)\n",
        "\n",
        "        self.__print_summary(f'Final DataFrame shape: {self.df.shape}')\n",
        "\n",
        "    def map_sector_inv(self):\n",
        "        mapper = {i: sector for i, sector in enumerate(self.sector_mapper)}\n",
        "        self.df['Sector'] = self.df['Sector'].apply(lambda x: mapper[x])\n",
        "\n",
        "    def get_intersecting_columns(self):\n",
        "        df1 = self.dfs[0].columns\n",
        "        df2 = self.dfs[1].columns\n",
        "        df3 = self.dfs[2].columns\n",
        "        df4 = self.dfs[3].columns\n",
        "        df5 = self.dfs[4].columns\n",
        "\n",
        "        self.intersec_cols = df1 & df2 & df3 & df4 & df5\n",
        "        print(f'Found {len(self.intersec_cols)} intersecting columns!')\n",
        "\n",
        "    def concat_intersecting_dfs(self):\n",
        "        df1 = self.dfs[0][self.intersec_cols]\n",
        "        df2 = self.dfs[1][self.intersec_cols]\n",
        "        df3 = self.dfs[2][self.intersec_cols]\n",
        "        df4 = self.dfs[3][self.intersec_cols]\n",
        "        df5 = self.dfs[4][self.intersec_cols]\n",
        "\n",
        "        df = pd.concat([df1, df2, df3, df4, df5])\n",
        "        print(f'Concatenated DataFrame into shape: {df.shape}')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def __print_summary(self, info: str):\n",
        "        self.__print_sep()\n",
        "        print(info)\n",
        "        self.__print_sep(60, '~', '\\n')\n",
        "\n",
        "    def __print_sep(self, n: int = 60, c: str = '-', nl: str = ''):\n",
        "        print(n * c + nl)\n",
        "\n",
        "    def __print_header(self, header: str):\n",
        "        rem = 60 - 25 - 2 - len(header)\n",
        "        print(f'{25 * \"~\"} {header} {rem * \"~\"}')"
      ],
      "metadata": {
        "id": "Qb_W1JWaj498"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare approaches"
      ],
      "metadata": {
        "id": "HNHa5JNemgs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set config dictionary"
      ],
      "metadata": {
        "id": "KhDP7N_Bs9Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_dict = {\n",
        "    2014: {\n",
        "        'nans_thres': 0.08, \n",
        "        'zeros_thres': 0.08,\n",
        "        'cut_lower': 0.01,\n",
        "        'cut_upper': 0.99,\n",
        "        'IForest_contamination': 0.02\n",
        "    },\n",
        "    2015: {\n",
        "        'nans_thres': 0.10, \n",
        "        'zeros_thres': 0.08,\n",
        "        'cut_lower': 0.01,\n",
        "        'cut_upper': 0.99,\n",
        "        'IForest_contamination': 0.02\n",
        "    },\n",
        "    2016: {\n",
        "        'nans_thres': 0.16, \n",
        "        'zeros_thres': 0.08,\n",
        "        'cut_lower': 0.01,\n",
        "        'cut_upper': 0.99,\n",
        "        'IForest_contamination': 0.02\n",
        "    },\n",
        "    2017: {\n",
        "        'nans_thres': 0.16, \n",
        "        'zeros_thres': 0.08,\n",
        "        'cut_lower': 0.01,\n",
        "        'cut_upper': 0.99,\n",
        "        'IForest_contamination': 0.02\n",
        "    },\n",
        "    2018: {\n",
        "        'nans_thres': 0.08, \n",
        "        'zeros_thres': 0.08,\n",
        "        'cut_lower': 0.01,\n",
        "        'cut_upper': 0.99,\n",
        "        'IForest_contamination': 0.02\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "4OwwwrQwgiO-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 1\n",
        "\n",
        "1.   *NaN threshold*\n",
        "2.   *zero threshold*\n",
        "3.   *mean imputation*\n",
        "4.   *outlier quantile cut*\n",
        "\n"
      ],
      "metadata": {
        "id": "otz1ng9emkbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = pd.read_csv(\"/2014_Financial_Data.csv\")\n",
        "print(f.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lni6vyS2hqx6",
        "outputId": "9e47e730-1375-4209-e294-18dbd837b4d0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Unnamed: 0       Revenue  Revenue Growth  Cost of Revenue  Gross Profit  \\\n",
            "0         PG  7.440100e+10         -0.0713     3.903000e+10  3.537100e+10   \n",
            "1       VIPS  3.734148e+09          1.1737     2.805625e+09  9.285226e+08   \n",
            "2         KR  9.837500e+10          0.0182     7.813800e+10  2.023700e+10   \n",
            "3        RAD  2.552641e+10          0.0053     1.820268e+10  7.323734e+09   \n",
            "4        GIS  1.790960e+10          0.0076     1.153980e+10  6.369800e+09   \n",
            "\n",
            "   R&D Expenses  SG&A Expense  Operating Expenses  Operating Income  \\\n",
            "0  0.000000e+00  2.146100e+10        2.146100e+10      1.391000e+10   \n",
            "1  1.083303e+08  3.441414e+08        7.939267e+08      1.345959e+08   \n",
            "2  0.000000e+00  1.519600e+10        1.751200e+10      2.725000e+09   \n",
            "3  0.000000e+00  6.561162e+09        6.586482e+09      7.372520e+08   \n",
            "4  0.000000e+00  3.474300e+09        3.412400e+09      2.957400e+09   \n",
            "\n",
            "   Interest Expense  ...  Receivables growth  Inventory Growth  Asset Growth  \\\n",
            "0      7.090000e+08  ...             -0.0187           -0.0217        0.0359   \n",
            "1      1.214869e+07  ...                 NaN               NaN           NaN   \n",
            "2      4.430000e+08  ...              0.0618            0.0981        0.1886   \n",
            "3      4.245910e+08  ...              0.0211           -0.0510       -0.0189   \n",
            "4      3.024000e+08  ...              0.0257            0.0090        0.0215   \n",
            "\n",
            "   Book Value per Share Growth  Debt Growth  R&D Expense Growth  \\\n",
            "0                       0.0316       0.1228              0.0000   \n",
            "1                          NaN          NaN              1.6484   \n",
            "2                       0.3268       0.2738              0.0000   \n",
            "3                       0.1963      -0.0458              0.0000   \n",
            "4                       0.0274       0.1025              0.0000   \n",
            "\n",
            "   SG&A Expenses Growth              Sector  2015 PRICE VAR [%]  Class  \n",
            "0               -0.1746  Consumer Defensive           -9.323276      0  \n",
            "1                1.7313  Consumer Defensive          -25.512193      0  \n",
            "2                0.0234  Consumer Defensive           33.118297      1  \n",
            "3               -0.0060  Consumer Defensive            2.752291      1  \n",
            "4               -0.0220  Consumer Defensive           12.897715      1  \n",
            "\n",
            "[5 rows x 225 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_1 = DataSet(config_dict=config_dict,\n",
        "                    nan_handler = ['threshold'],\n",
        "                    zero_handler = ['threshold'],\n",
        "                    imputation_handler = 'mean',\n",
        "                    outlier_handler = ['quantile'])\n",
        "\n",
        "# generate DataFrame\n",
        "df_1 = dataset_1.create_dataset()\n",
        "\n",
        "# use full df for cross validation vanilla test\n",
        "X_1, y_1 = df_1.drop(columns=['Class']), df_1['Class']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XyMm5zQ8gZnV",
        "outputId": "2a6c533b-5ea8-470f-f40a-2f1ee6e8f05e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2014 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2014_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (3808, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% NaNs - 89 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 60 columns dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 150 - Current shape: (3808, 74)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing NaNs with mean\n",
            "Dropping rows that have no information about revenue - 44 rows dropped - COMPLETE\n",
            "8163 NaNs imputed - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 76 rows dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3688, 74)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2015 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2015_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4120, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 10% NaNs - 68 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 67 columns dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 136 - Current shape: (4120, 88)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing NaNs with mean\n",
            "Dropping rows that have no information about revenue - 67 rows dropped - COMPLETE\n",
            "16665 NaNs imputed - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 82 rows dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3971, 88)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2016 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2016_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4797, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 16% NaNs - 97 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 61 columns dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 159 - Current shape: (4797, 65)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing NaNs with mean\n",
            "Dropping rows that have no information about revenue - 489 rows dropped - COMPLETE\n",
            "9768 NaNs imputed - 10 NaNs left - COMPLETE\n",
            "Dropping rows with remaining NaNs - 1 row(s) dropped - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 88 rows dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4219, 65)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2017 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2017_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4960, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 16% NaNs - 145 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 31 columns dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 177 - Current shape: (4960, 47)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing NaNs with mean\n",
            "Dropping rows that have no information about revenue - 525 rows dropped - COMPLETE\n",
            "5506 NaNs imputed - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 90 rows dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4345, 47)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2018 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2018_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4392, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% NaNs - 62 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 70 columns dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 133 - Current shape: (4392, 91)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing NaNs with mean\n",
            "Dropping rows that have no information about revenue - 46 rows dropped - COMPLETE\n",
            "14056 NaNs imputed - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 88 rows dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4258, 91)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "############################################################\n",
            "\n",
            "Found 42 intersecting columns!\n",
            "Concatenated DataFrame into shape: (20481, 42)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '/2018 PRICE VAR [%]'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-f64cb5de49a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# generate DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# use full df for cross validation vanilla test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-229e70fabc90>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# factorize 'Sector' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorize_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# avoid encoding issues with feature names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-229e70fabc90>\u001b[0m in \u001b[0;36mfactorize_col\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sector'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msector_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sector'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sector'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'/{self.year} PRICE VAR [%]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0msp500_2015_buy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Class\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'/{self.year} PRICE VAR [%]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0msp500_2015_sell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Class\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         df.loc[(df[f'/{self.year} PRICE VAR [%]'] >= sp500_2015_sell) & \n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '/2018 PRICE VAR [%]'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vanilla(X_1, y_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw4By1i5oz-e",
        "outputId": "f213f019-6017-48c9-e6f0-1014c5a55733"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for ETC: 0.7009426914891994\n",
            "Accuracy for RFC: 0.7019196115297779\n",
            "Accuracy for GBC: 0.6988923869599708\n",
            "Accuracy for LGBMC: 0.7049952828479681\n",
            "Accuracy for XGBC: 0.6985992632757201\n",
            "Accuracy for CBC: 0.7043116771799488\n",
            "Accuracy for KNN: 0.6094914867204662\n",
            "Accuracy for SGDC: 0.5520240968048267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 2\n",
        "\n",
        "\n",
        "1.   *zero threshold*\n",
        "2.   *replace zeros with NaNs*\n",
        "3.   *KNN imputation*\n",
        "4.   *outlier quantile cut*\n",
        "5.   *Isolation Forest outlier handling*\n",
        "\n"
      ],
      "metadata": {
        "id": "H4SGEHkIo7kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_2 = DataSet(config_dict=config_dict,\n",
        "                    nan_handler = [],\n",
        "                    zero_handler = ['threshold', 'replace'],\n",
        "                    imputation_handler = 'KNN',\n",
        "                    outlier_handler = ['quantile', 'IForest'])\n",
        "\n",
        "# generate DataFrame\n",
        "df_2 = dataset_2.create_dataset()\n",
        "\n",
        "# use full df for cross validation vanilla test\n",
        "X_2, y_2 = df_2.drop(columns=['Class']), df_2['Class']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNjNTA_PqSil",
        "outputId": "4d357811-faf3-46fd-91f9-42716f5cd5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2014 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2014_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (3808, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 87 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 7842 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 88 - Current shape: (3808, 136)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 77059 NaNs with KNN - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 78 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 75 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3655, 136)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2015 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2015_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4120, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 84 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 8989 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 85 - Current shape: (4120, 139)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 87684 NaNs with KNN - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 84 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 81 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3955, 139)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2016 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2016_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4797, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 84 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 10615 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 85 - Current shape: (4797, 139)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 150611 NaNs with KNN - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 96 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 94 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4607, 139)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2017 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2017_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4960, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 84 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 10279 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 85 - Current shape: (4960, 139)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 158663 NaNs with KNN - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 100 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 98 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4762, 139)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2018 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2018_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4392, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 87 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 8561 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 88 - Current shape: (4392, 136)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 75465 NaNs with KNN - 0 NaNs left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 88 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 87 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4217, 136)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "############################################################\n",
            "\n",
            "Found 133 intersecting columns!\n",
            "Concatenated DataFrame into shape: (21196, 133)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vanilla(X_2, y_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "MKZb3loZrFCr",
        "outputId": "0eb97889-a384-4db9-efc3-3c97d2961e64"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-13689e72d3a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_vanilla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 3\n",
        "\n",
        "1.   *replace zeros with NaNs*\n",
        "3.   *XGB imputation*\n",
        "4.   *outlier quantile cut*\n",
        "5.   *Isolation Forest outlier handling*\n"
      ],
      "metadata": {
        "id": "cKkOx65LrH0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_3 = DataSet(config_dict=config_dict,\n",
        "                    nan_handler = [],\n",
        "                    zero_handler = ['replace'],\n",
        "                    imputation_handler = 'XGB',\n",
        "                    outlier_handler = ['quantile', 'IForest'])\n",
        "\n",
        "# generate DataFrame\n",
        "df_3 = dataset_3.create_dataset()\n",
        "\n",
        "# use full df for cross validation vanilla test\n",
        "X_3, y_3= df_3.drop(columns=['Class']), df_3['Class']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-dLVG_8rceJ",
        "outputId": "c56fed79-c7c5-4f65-d090-91d7b809ce1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2014 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2014_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (3808, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Replacing zeros with NaNs - 113537 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 1 - Current shape: (3808, 223)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 214640 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "824 NaNs left - COMPLETE\n",
            "Removing 3 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 78 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 75 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3655, 217)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2015 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2015_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4120, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Replacing zeros with NaNs - 120324 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 1 - Current shape: (4120, 223)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 234672 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "5825 NaNs left - COMPLETE\n",
            "Removing 5 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 84 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 81 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3955, 217)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2016 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2016_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4797, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Replacing zeros with NaNs - 130018 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 1 - Current shape: (4797, 223)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 342567 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "3577 NaNs left - COMPLETE\n",
            "Removing 4 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 96 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 94 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4607, 218)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2017 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2017_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4960, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Replacing zeros with NaNs - 132119 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 1 - Current shape: (4960, 223)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 358388 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "0 NaNs left - COMPLETE\n",
            "Removing 0 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 100 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 98 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4762, 222)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2018 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2018_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4392, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Replacing zeros with NaNs - 131782 zeros replaced - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 1 - Current shape: (4392, 223)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 229080 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "0 NaNs left - COMPLETE\n",
            "Removing 0 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 88 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 87 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4217, 222)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "############################################################\n",
            "\n",
            "Found 215 intersecting columns!\n",
            "Concatenated DataFrame into shape: (21196, 215)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vanilla(X_3, y_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOvsf0o5ro21",
        "outputId": "12ea749d-177c-4b98-8cf6-79792b831e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for ETC: 0.710086873016509\n",
            "Accuracy for RFC: 0.71636189337998\n",
            "Accuracy for GBC: 0.7087662295753271\n",
            "Accuracy for LGBMC: 0.7132482073468733\n",
            "Accuracy for XGBC: 0.7082472058646797\n",
            "Accuracy for CBC: 0.7170223152487905\n",
            "Accuracy for KNN: 0.6530477106117053\n",
            "Accuracy for SGDC: 0.5499137390003872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 4\n",
        "\n",
        "1.   *NaN threshold*\n",
        "2.   *zero threshold*\n",
        "3.   *replace zeros with NaNs*\n",
        "4.   *NaN threshold again*\n",
        "5.   *KNN imputation*\n",
        "6.   *outlier quantile cut*\n",
        "7.   *Isolation Forest outlier handling*"
      ],
      "metadata": {
        "id": "Khb0Dm11rpyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_4 = DataSet(config_dict=config_dict,\n",
        "                    nan_handler = ['threshold'],\n",
        "                    zero_handler = ['threshold', 'replace', 'drop_again'],\n",
        "                    imputation_handler = 'XGB',\n",
        "                    outlier_handler = ['quantile', 'IForest'])\n",
        "\n",
        "# generate DataFrame\n",
        "df_4 = dataset_4.create_dataset()\n",
        "\n",
        "# use full df for cross validation vanilla test\n",
        "X_4, y_4= df_4.drop(columns=['Class']), df_4['Class']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9STn0QbfsV6D",
        "outputId": "f9a33aa5-84a3-40ff-e93b-65c157d0c4d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2014 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2014_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (3808, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% NaNs - 89 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 60 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 4529 zeros replaced - COMPLETE\n",
            "Dropping columns with more than 8% NaNs - 14 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 164 - Current shape: (3808, 60)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 9864 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "169 NaNs left - COMPLETE\n",
            "Removing 1 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 78 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 75 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3655, 59)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2015 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2015_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4120, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 10% NaNs - 68 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 67 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 5357 zeros replaced - COMPLETE\n",
            "Dropping columns with more than 10% NaNs - 17 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 153 - Current shape: (4120, 71)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 18061 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "291 NaNs left - COMPLETE\n",
            "Removing 1 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 84 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 81 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (3955, 69)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2016 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2016_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4797, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 16% NaNs - 97 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 61 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 5318 zeros replaced - COMPLETE\n",
            "Dropping columns with more than 16% NaNs - 17 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 176 - Current shape: (4797, 48)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 29474 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "751 NaNs left - COMPLETE\n",
            "Removing 1 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 96 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 94 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4607, 47)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2017 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2017_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4960, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 16% NaNs - 145 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 31 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 3914 zeros replaced - COMPLETE\n",
            "Dropping columns with more than 16% NaNs - 11 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 188 - Current shape: (4960, 36)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 21796 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "0 NaNs left - COMPLETE\n",
            "Removing 0 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 100 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 98 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4762, 36)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ LOADING 2018 ~~~~~~~~~~~~~~~~~~~~~\n",
            "Loading 2018_Financial_Data.csv into a DataFrame - COMPLETE\n",
            "Dropping rows with NaNs only - 0 rows dropped - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Initial DataFrame shape: (4392, 224)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING NANS ~~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% NaNs - 62 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLING ZEROS ~~~~~~~~~~~~~~~~~~~\n",
            "Dropping columns with more than 8% zeros - 70 columns dropped - COMPLETE\n",
            "Replacing zeros with NaNs - 6019 zeros replaced - COMPLETE\n",
            "Dropping columns with more than 8% NaNs - 18 columns dropped - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "------------------------------------------------------------\n",
            "Total amount of columns dropped: 151 - Current shape: (4392, 73)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ IMPUTE NANS ~~~~~~~~~~~~~~~~~~~~~~\n",
            "Imputing 15473 NaNs with XGB - NaNImputer(conservative = False, n_feats = 10,            \n",
            "           fix_string_nans = True, verbose = False,                \n",
            "           multiprocessing_load = 1, fill_nans_in_pure_text = True,                    \n",
            "           drop_empty_cols = True, drop_nan_cols_with_constant = True                        \n",
            "           feature_selection = correlation)\n",
            "\n",
            "Impute sequentially on a single core\n",
            "\n",
            "2650 NaNs left - COMPLETE\n",
            "Removing 11 columns such that 0 NaNs are left - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ HANDLE OUTLIERS ~~~~~~~~~~~~~~~~~~\n",
            "Dropping rows with a price variance outside the 0.01 - 0.99 quantile range - 88 rows dropped - COMPLETE\n",
            "Using Isolation Forest to detect outliers - 87 outliers removed - COMPLETE\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~ SCALING DATA ~~~~~~~~~~~~~~~~~~~~~\n",
            "Scaling data - COMPLETE\n",
            "------------------------------------------------------------\n",
            "Final DataFrame shape: (4217, 61)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "############################################################\n",
            "############################################################\n",
            "\n",
            "Found 28 intersecting columns!\n",
            "Concatenated DataFrame into shape: (21196, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vanilla(X_4, y_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay4t_2VOsf8p",
        "outputId": "10a23848-efa2-4e07-a689-f37107727df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for ETC: 0.6987644491625383\n",
            "Accuracy for RFC: 0.7015478129854407\n",
            "Accuracy for GBC: 0.6977734714043452\n",
            "Accuracy for LGBMC: 0.702963107621502\n",
            "Accuracy for XGBC: 0.6968300084124504\n",
            "Accuracy for CBC: 0.7039069489511144\n",
            "Accuracy for KNN: 0.6524813946863579\n",
            "Accuracy for SGDC: 0.5021214063480619\n"
          ]
        }
      ]
    }
  ]
}